---
title: QEUR23_SIWC1 : 簡単なマルチエージェント(Qwen-7b)を構築する
date: 2025-03-25
tags: ["QEUシステム", "メトリックス", "Python言語", "Unsloth", "LLM", "データセット", "BONSAI", "LangGraph"]
excerpt: あたらしいLLMの学習体系を確立する
---

## QEUR23_SIWC1 : 簡単なマルチエージェント(Qwen-7b)を構築する

## ～ 外部のllmプラットフォームとの連結が不可欠 ～

### ・・・ 前回のつづきです ・・・

C部長： “LANGGRAPHには、他にもいろいろな使い方ができるんでしょう？”

QEU:FOUNDER ： “もちろん！ただし、現時点でLANGGRAPHのメリットを議論するのは早計ですね。もちろん、LANGGRAPHの持つ本当の強みはC国で一生懸命LANGGRAPHを勉強しているC国の学習者は知っているんでしょう。そうでなければ、あれほどのブームにはなりません。さあ、次に進みましょう。さしあたり、当面の目標は、「QEU用のマルチ・エージェント」を作るつもりでいきます。”

![imageSWC1-2-1](/2025-03-25-QEUR23_SIWC1/imageSWC1-2-1.jpg) 

QEU:FOUNDER ： “まずは、非常に簡単な事例をやってみます。この参考事例（↑）のコードをもとに、かなり内容を変更しました。”

D先生： “何を変更したのですか？”

QEU:FOUNDER ： “変更する内容は、「実は一言」なのです。OPENAIやCOHEREのAPIを敢えて使わず、**HuggingfaceからオープンLLMをダウンロードして使ってみました**。これは、一見は簡単そうに見えて、すごく大変でした。”

```python
# -----
# オープンモデルを使います
from langchain_community.llms import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

# 1. モデルのセットアップ
model_name = "Qwen/Qwen2.5-7B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=512,
    temperature=0.1,
    repetition_penalty=1.15,
    pad_token_id=tokenizer.eos_token_id,
    return_text=True       # テキスト形式での出力を有効化
)

llm = HuggingFacePipeline(pipeline=pipe)

```

D先生： “ん！？これは、いたって普通のLLMモデルの導入に見えます。”

QEU:FOUNDER ： “モデル出力とLangChainとの相性のマッチングが、なかなかに難しいのです。ここでは、**パイプライン**を使いましたが、それ以外にも大変に多くの修正を繰り返しました。それでは、次に行きましょう。”

```python
# ---
# Creating agents
import requests
from langchain.prompts import PromptTemplate

# Creating the first analysis agent to check the prompt structure
# This print part helps you to trace the graph decisions
def analyze_question(state):
    prompt = PromptTemplate.from_template("""
    Question: {input}

    Is this a generic or technical coding question? Please classify it to simply answer 'code' or 'general' ONLY:
    """)
    chain = prompt | llm
    response = chain.invoke({"input": state["input"]})
    print("---  analyze_question:prompt ---")
    print(prompt)
    print("---  analyze_question:response ---")
    print(response)
    return {
        "decision": "code" if ("code" in response) or ("Code" in response) else "general",
        "input": state["input"]
    }

# Creating the code agent that could be way more technical
def answer_code_question(state):
    # ---
    prompt = PromptTemplate.from_template(
        "You are a software engineer. Answer this question with step by steps details : {input}"
    )
    chain = prompt | llm
    response = chain.invoke({"input": state["input"]})
    print("---  answer_code_question:prompt ---")
    print(prompt)
    print("---  answer_code_question:response ---")
    print(response)
    if isinstance(response, dict) and "generated_text" in response:
        return {"output": response["generated_text"]}
    else:
        return {"output": response}

# Creating the generic agent
def answer_generic_question(state):
    # ---
    prompt = PromptTemplate.from_template(
        "Give a general and concise answer to the question: {input}"
    )
    chain = prompt | llm
    response = chain.invoke({"input": state["input"]})
    print("---  answer_generic_question:prompt ---")
    print(prompt)
    print("---  answer_generic_question:response ---")
    print(response)
    if isinstance(response, dict) and "generated_text" in response:
        return {"output": response["generated_text"]}
    else:
        return {"output": response}

# ---
# Building the Graph
from langgraph.graph import StateGraph, END
from typing import Annotated, TypedDict

#You can precise the format here which could be helpfull for multimodal graphs
class AgentState(TypedDict):
    input: str
    output: str
    decision: str

# Here is a simple 3 steps graph that is going to be working in the below "decision" condition
# これは、以下の「決定」条件で機能するシンプルな3ステップのグラフです。
def create_graph():
    workflow = StateGraph(AgentState)

    workflow.add_node("analyze", analyze_question)
    workflow.add_node("code_agent", answer_code_question)
    workflow.add_node("generic_agent", answer_generic_question)

    # 条件分岐前のバリデーション
    workflow.add_conditional_edges(
        "analyze",
        lambda x: x["decision"] if x["decision"] in ["code", "general"] else "general",
        {"code": "code_agent", "general": "generic_agent"}
    )

    workflow.set_entry_point("analyze")
    workflow.add_edge("code_agent", END)
    workflow.add_edge("generic_agent", END)

    return workflow.compile()

# ---
import requests
from langgraph.graph import Graph
from IPython.display import Image, display

runner = create_graph()
display(Image(runner.get_graph().draw_mermaid_png()))

```

QEU:FOUNDER ： “このエージェントには、LLMが3つ必要です。むりやり、１つのLLMを使わせていますがね。”

![imageSWC1-2-2](/2025-03-25-QEUR23_SIWC1/imageSWC1-2-2.jpg) 

D先生： “確かにマルチ・エージェントの構造になっています。「analyze」で振り替えているわけです。”

QEU:FOUNDER ： “analyze用エージェントの命令だけをポップアップしましょう。”

```python
prompt = PromptTemplate.from_template("""
Question: {input}

Is this a generic or technical coding question? Please classify it to simply answer 'code' or 'general' ONLY:
""")

```

D先生： “すごく簡単な質問です。この質問は**「分類タスク」**に当たりますね。そういえば、今回は7bのモデルを使いました。”

QEU:FOUNDER ： “A100のGPUを使って、簡単に高速で動かしたかったんです。つづきに行きましょう。”

```python
# ---
# Launching the Program
# ---
import requests
from typing import Annotated, TypedDict
from langgraph.graph import StateGraph, END

class UserInput(TypedDict):
    input: str
    continue_conversation: bool

def get_user_input(state: UserInput) -> UserInput:
    user_input = input("\nEnter your question (or 'q' to quit) : ")
    return {
        "input": user_input,
        "continue_conversation": user_input.lower() != 'q'
    }

# create_graphからの継承
def process_question(state: UserInput):
    graph = create_graph()
    result = graph.invoke({"input": state["input"]})
    print("\n--- Final answer ---")
    print(result["output"])
    return state

def create_conversation_graph():
    workflow = StateGraph(UserInput)

    workflow.add_node("get_input", get_user_input)
    workflow.add_node("process_question", process_question)

    workflow.set_entry_point("get_input")

    workflow.add_conditional_edges(
        "get_input",
        lambda x: "continue" if x["continue_conversation"] else "end",
        {
            "continue": "process_question",
            "end": END
        }
    )

    workflow.add_edge("process_question", "get_input")

    return workflow.compile()

# ---
import requests
from langgraph.graph import Graph
from IPython.display import Image, display

workflow = create_conversation_graph()
display(Image(workflow.get_graph().draw_mermaid_png()))

```

D先生： “おっと！ここにもグラフがついてきています。”

![imageSWC1-2-3](/2025-03-25-QEUR23_SIWC1/imageSWC1-2-3.jpg) 

QEU:FOUNDER ： “繰り返して質問を続けるような設定ですね。「q」のキーで循環が止まります。”

```python
# ---
conversation_graph = create_conversation_graph()

response = conversation_graph.invoke({"input": "", "continue_conversation": True})
print(response)

```

QEU:FOUNDER ： “それでは実行してみましょう。まずは、**「一般質問」の場合**を見てみましょう。”

![imageSWC1-2-4](/2025-03-25-QEUR23_SIWC1/imageSWC1-2-4.jpg) 

D先生： “富士山・・・。この質問は、あえてE語にしなかったのですね。まあ、LLMは一般質問だと認識しているようです。それにしても、回答文が長いなあ・・・。”

QEU:FOUNDER ： “7bモデルなので、「気が利かない」のが根本原因です。ちゃんとファインチューニングをしておけば、このようなことにはならないと思います。”

D先生： “3つのエージェントに別々の機能を期待すべきですね。”

QEU:FOUNDER ： “まあ、これはPROMPT_ENGINEERINGの限界です。それでは、**コードの出力**に行ってみましょう。”

![imageSWC1-2-5](/2025-03-25-QEUR23_SIWC1/imageSWC1-2-5.jpg) 

D先生： “一般質問と、ほぼ同じ感じの「もっさりした（キレのない）」回答です。しかし、そもそものツッコミとして、このLLMが「コード生成用」なのかと・・・（笑）。”

QEU:FOUNDER ： “APIを使って外からエージェントからの出力情報を取り込むタイプになっていないと、本当の意味での「マルチ・エージェント」にならないんです。あとねえ・・・。あと、やっぱりA100でも、GPUパワーがまだ弱いのでテキスト生成速度が遅いです。”

D先生： “連続してエージェントを動かす、このタイプのシステムにはAPIサービスを使うしかないんでしょうね。ただし、どのようなAPIサービスでも良いとも思えません。いつものようにCohereを使いますか？”

QEU:FOUNDER ： “Cohereは、モデルのバリエーションが少ないし、ファインチューニングにも制約があります。あそこの良いところはEmbeddingとRerankが強いところです。しかしながら、QwQ-32bを見ればわかるように、最近はプロプライエタリLLMとオープンLLMの実力差が小さくなってきているんです。オープンLLMにファインチューニングをして、あくまで自分用で使うのであれば、その差はごく僅かでしょう。”

D先生： “う～ん・・・。どのAPIがいいんだろう・・・。LangChainが、そのサービスをサポートしていないといけないし・・・。”

## ～ まとめ ～

### ・・・ 前回の続きです ・・・

QEU:FOUNDER ： “このセリフ(↓)、この方が言ったの？当時は別に有名人でもないのに・・・。”

![imageSWC1-2-6](/2025-03-25-QEUR23_SIWC1/imageSWC1-2-6.jpg) 

D先生： “へえそうなんだ・・・。例のメディアが出自なのか・・・。C部長が好きな「あのメディア」です。”

### 自分や自国が永遠に凄い、あるいは永遠に世界の上位だと思い込んでいる人は、早く気づくべきことがあります。

- 1／世界は常に変わり続けています。
- 2／永遠に強い国として存在し続ける国はありません。
- 3／上下の地位はいつでも逆転する可能性があります。
- 4／あなたが凄いと思っていることも、やがて凄くなくなるでしょう。
- 5／例として挙げますと、かつて「軽蔑」されていた中国も今は変わりました。世界の警察と呼ばれたアメリカも、変わりつつあります。
- 6／あなたが「軽蔑」している国や人々も、日々変化しています。
- 7／その「軽蔑」している国や人々の下で、あなたが働く日が来る可能性も充分にあります。
- 8／歴史を振り返れば、傲慢や過信はいつも破滅の前触れです。謙虚さを持ち、他国や他者を尊重することが、未来への備えとなるでしょう。

C部長 : “いやいや・・・。このコメント（↑）のように、なかなかに勉強になる瞬間もありますよ。”

[![MOVIE1](http://img.youtube.com/vi/WtmqH8iBgYI/0.jpg)](http://www.youtube.com/watch?v=WtmqH8iBgYI "【衰退の本質】ソニーとの勝負の分かれ目／カリスマからの脱却／中村体制のプラズマ敗戦が決定的／破壊に終始／日産との共通点／アンテナが低い／")

QEU:FOUNDER ： “あのメディアは、何はともかくとして世界（視野）が広がるからねえ。このV国のお方の発言には、いままでも勉強をさせてもらいました。感謝です。さて、それにしても、あの地方の衰退、一体、どうなっているんでしょうねえ。憧れだったのに、ホーミタイッ・・・。”

![imageSWC1-2-7](/2025-03-25-QEUR23_SIWC1/imageSWC1-2-7.jpg) 

QEU:FOUNDER ： “あの地方って「クセがある」んですよね。なんで、自分の商売の成功を一般化して、「哲学」にしようとするのか・・・。1社じゃないんだよねえ・・・。どうして、あんなに多くの会社が「同じ傾向」をもつのか？”

![imageSWC1-2-8](/2025-03-25-QEUR23_SIWC1/imageSWC1-2-8.jpg) 

C部長 : “これが地域文化なのでしょうね。しかし、「勤勉、倹約、正直」を貴ぶ道徳教が、どうして近年になって、新自由主義とか搾取の正当化に移行してしまうのか・・・。”

![imageSWC1-2-9](/2025-03-25-QEUR23_SIWC1/imageSWC1-2-9.jpg) 

QEU:FOUNDER ： “興味深いですね。”
