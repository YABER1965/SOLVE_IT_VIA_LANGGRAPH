---
title: QEUR23_SIWC5 - 閑話休題～クエリとドキュメントの一致性を評価する
date: 2025-04-05
tags: ["QEUシステム", "メトリックス", "Python言語", "Unsloth", "LLM", "データセット", "BONSAI", "LangGraph"]
excerpt: あたらしいLLMの学習体系を確立する
---

## QEUR23_SIWC5 - 閑話休題～クエリとドキュメントの一致性を評価する

## ～ エージェントの自動化には「評価の自動化」が不可欠です ～

### ・・・ 前回のつづきです ・・・

D先生： “さて、RAG推論の結果はどう出るか・・・。”

![imageSWC1-6-1](/2025-04-05-QEUR23_SIWC5/imageSWC1-6-1.jpg) 

content='外観検査と画像判別の技術上の差異は、CNN（Convolutional Neural Network）とViT（Vision Transformer）の違いである。CNNは畳み込みによって画像情報を集約して特徴ベクトルを作るため、個体判別に優れているが、被検査体の特徴が空間的に分離した場合の判別の精度は良くない。一方、ViTは自然言語処理から派生した技術であり、空間的な制限がないため、外観検査用に優れている可能性がある。'
 additional_kwargs={} 
response_metadata={'token_usage': {'prompt_tokens': 356, 'total_tokens': 486, 'completion_tokens': 130}, 
'model_name': 'accounts/fireworks/models/llama-v3p3-70b-instruct', 
'system_fingerprint': '', 
'finish_reason': 'stop', 'logprobs': None} 
usage_metadata={'input_tokens': 356, 'output_tokens': 130, 'total_tokens': 486}

D先生： “なるほどね。クエリの文に、「技術的に・・・」という言葉を入れたからこうなったでしょうね。このコメント自体は間違ってはいません。「画像分類は、全体的に少しの変化がある」のに対し、**「異常検出は99％は全く同じ、1%だけ異なる」** という風になれば、満点だったのだが・・・。“

QEU:FOUNDER  ： “D先生が求めている回答は、「事象の差異」といいます。「技術の差異」ではないですよ。さて、次のステップに進みましょう。その前にQEUシステムからのCMです。”

![imageSWC1-6-2](/2025-04-05-QEUR23_SIWC5/imageSWC1-6-2.jpg) 

QEU:FOUNDER ： “我々が今推進しているのは、上図の**「EVERYBODY IS CREATIVE CLASS」**に属します。前回に述べたように、カスタム設計されたエージェントを使えば**ハードウェア・バイパスを使っても「知能爆発（ポジティブ・フィードバック）」は可能です**。もちろん、その場合には個人、企業規模の小さな爆発にはなるがね。”

![imageSWC1-6-3](/2025-04-05-QEUR23_SIWC5/imageSWC1-6-3.jpg) 

D先生: “そして、そのときの、「あるべきCREATIVE CLASS」とは、データを獲得できる「こういう人たち（↑）」だと・・・。今回は、この遠大なテーマを、とりあえず**「SOLVE IT WITH CODE」の枠内で行っています**が、うまく行ったら、この理念も少し書き換えを行う必要がありますね。”

QEU:FOUNDER ： “それでは本題に入ります。今回やることは**「評価と分岐」**です。複雑なエージェントでは、その情報処理の節目で評価をして、その結果に応じて、種々のLLMモデルやプロンプトを使う必要がありますからね。”

![imageSWC1-6-4](/2025-04-05-QEUR23_SIWC5/imageSWC1-6-4.jpg) 

D先生：“基本的には、前回の続きでしょう？”

QEU:FOUNDER ： “もちろん。FIREWORKのモデルを使いますし、ベクトル・ストアは前回に作成したものを使います。それでは、プログラムをドン！”

```python
##############################
# 今回のテーマは「評価と分岐」です！
##############################
# ---
from langchain_community.vectorstores import Chroma
from langchain_cohere import CohereEmbeddings

# エンべディングモデルの設定
embeddings = CohereEmbeddings(
    model="embed-multilingual-v3.0",
)

# ベクトル・ストアのファイルパス
VECTOR_STORE_PATH = "drive/MyDrive/chroma_db"

# ---
# 既存データベースの読み込み
if os.path.exists(VECTOR_STORE_PATH):
    print("既存のChromaデータベースを読み込み中...")
    vectorstore = Chroma(persist_directory=VECTOR_STORE_PATH, embed-ding_function=embeddings)
else:
    # 新規作成処理
    print("エラーが発生しました")
    #exit

# ---
### Set LLM
from langchain_fireworks import ChatFireworks

# Initialize a Fireworks chat model
llm = ChatFireworks(
    model="accounts/fireworks/models/llama-v3p3-70b-instruct",
    temperature=0.0,
    max_tokens=512
    )

# ---
from typing import Literal
from pydantic import BaseModel, Field
from langchain_core.messages import HumanMessage, SystemMessage

# ---
# メイン処理
# ベクトル・ストアの準備
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 5}  # 上位5件を取得
)

# ---
### Retrieval Grader 
from langchain_core.prompts import ChatPromptTemplate

# Data model
class GradeDocuments(BaseModel):
    """取得したドキュメントの関連性チェックのためのバイナリ・スコア。"""

    binary_score: str = Field(description="文書は質問に関連している, 'YES' or 'NO'")

# LLM with structured output
structured_llm_doc_grader = llm.with_structured_output(GradeDocuments)

# Doc grader instructions 
doc_grader_instructions = """あなたは、取得したドキュメントとユーザーの質問との関連性を評価する採点者です。
ドキュメントに質問に関連するキーワードまたは意味論的な意味(semantic meaning)が含まれている場合は、関連性があると採点します。
ドキュメントが質問に関連しているかどうかを示すために、「YES」または「NO」のバイナリ・スコアを付けてください。"""

# Grader prompt
doc_grader_prompt = "取得した文書はこちらです: \n\n {document} \n\n ユーザーの質問はこちらです: \n\n {question}"

# ---
# Post-processing
def format_docs(docs):
    len_docs = len(docs)
    doc_txt = ""
    print("len_docs: ", len_docs)
    for i in range(len_docs):
        doc_txt = doc_txt + docs[i].page_content + "\n"
        print("URL: " + docs[i].metadata['source'] + "\n")
    # ---
    # clearn-up
    doc_txt = doc_txt.replace("スキップしてメイン コンテンツに移動\n","\n")
    doc_txt = doc_txt.replace("このブログを検索\n","\n")
    doc_txt = doc_txt.replace("リンクを取得\n","\n")
    doc_txt = doc_txt.replace("Facebook\n","\n")
    doc_txt = doc_txt.replace("Pinterest\n","\n")
    doc_txt = doc_txt.replace("メール\n","\n")
    doc_txt = doc_txt.replace("他のアプリ\n","\n")
    doc_txt = doc_txt.replace("コメント\n","\n")
    doc_txt = doc_txt.replace("コメントを投稿\n","\n")
    doc_txt = doc_txt.replace("このブログの人気の投稿\n","\n")
    doc_txt = doc_txt.replace("\n\n\n\n","\n")
    doc_txt = doc_txt.replace("\n\n\n","\n")
    doc_txt = doc_txt.replace("\n\n","\n")
    
    return doc_txt

# ---
# Test用の関数
def evaluate_document(question1, question2):
    docs = retriever.invoke(question1)
    #doc_txt = docs[1].page_content
    doc_txt = format_docs(docs)
    doc_grader_prompt_formatted = doc_grader_prompt.format(document=doc_txt, ques-tion=question2)
    llm_doc_grader_input = [SystemMessage(content=doc_grader_instructions)] + [Human-Message(content=doc_grader_prompt_formatted)]
    return doc_txt, structured_llm_doc_grader.invoke(llm_doc_grader_input)

# ---
# 質問の実行(1)
query1 = "QEUシステムにおける外観検査法の特徴を教えてください。"

# Test router(1)
document, response = evaluate_document(query1, query1)
print(document)
print("-------")
print(response)

```

D先生：“今回は、新しいLangChain技術を使っています。**Structured output（構造化出力？）**と言うんですか？”

QEU:FOUNDER  ： “これが、最高に便利な機能なんだよなあ・・・。”

```python
# Data model
class GradeDocuments(BaseModel):
    """取得したドキュメントの関連性チェックのためのバイナリ・スコア。"""

    binary_score: str = Field(description="文書は質問に関連している, 'YES' or 'NO'")

# LLM with structured output
structured_llm_doc_grader = llm.with_structured_output(GradeDocuments)

```

QEU:FOUNDER  ： “例えば、今回のように回答にバイナリデータ（YES/NO）しか要らない場合、小生は、恥ずかしながら、その出力（YES/NO）を確実するためにLLMファインチューニングがいるものと思っていました。しかし、このstructured_outputの機能を使えば、とても簡単に回答を2値化できるんです。それでは、本例題の実行事例を見てみましょう。プログラムをドン！！”

```python
# ---
# Doc grader instructions 
doc_grader_instructions = """あなたは、取得したドキュメントとユーザーの質問との関連性を評価する採点者です。
ドキュメントに質問に関連するキーワードまたは意味論的な意味(semantic meaning)が含まれている場合は、関連性があると採点します。
ドキュメントが質問に関連しているかどうかを示すために、「YES」または「NO」のバイナリ・スコアを付けてください。"""

# Grader prompt
doc_grader_prompt = "取得した文書はこちらです: \n\n {document} \n\n ユーザーの質問はこちらです: \n\n {question}"

# ---
# Post-processing
def format_docs(docs):
    len_docs = len(docs)
    doc_txt = ""
    print("len_docs: ", len_docs)
    for i in range(len_docs):
        doc_txt = doc_txt + docs[i].page_content + "\n"
        print("URL: " + docs[i].metadata['source'] + "\n")
    # ---
    # clearn-up
    doc_txt = doc_txt.replace("スキップしてメイン コンテンツに移動\n","\n")
    doc_txt = doc_txt.replace("このブログを検索\n","\n")
    doc_txt = doc_txt.replace("リンクを取得\n","\n")
    doc_txt = doc_txt.replace("Facebook\n","\n")
    doc_txt = doc_txt.replace("Pinterest\n","\n")
    doc_txt = doc_txt.replace("メール\n","\n")
    doc_txt = doc_txt.replace("他のアプリ\n","\n")
    doc_txt = doc_txt.replace("コメント\n","\n")
    doc_txt = doc_txt.replace("コメントを投稿\n","\n")
    doc_txt = doc_txt.replace("このブログの人気の投稿\n","\n")
    doc_txt = doc_txt.replace("\n\n\n\n","\n")
    doc_txt = doc_txt.replace("\n\n\n","\n")
    doc_txt = doc_txt.replace("\n\n","\n")
    
    return doc_txt

# ---
# Test用の関数
def evaluate_document(question1, question2):
    docs = retriever.invoke(question1)
    #doc_txt = docs[1].page_content
    doc_txt = format_docs(docs)
    doc_grader_prompt_formatted = doc_grader_prompt.format(document=doc_txt, ques-tion=question2)
    llm_doc_grader_input = [SystemMessage(content=doc_grader_instructions)] + [Human-Message(content=doc_grader_prompt_formatted)]
    return doc_txt, structured_llm_doc_grader.invoke(llm_doc_grader_input)

```

D先生： “ここでは、推論用の関数「evaluate_document」がポイントですね。意図的に、**2つのクエリが入っています**。“

```python
# ---
# 質問の実行(1)
query1 = "QEUシステムにおける外観検査法の特徴を教えてください。"

# Test router(1)
document, response = evaluate_document(query1, query1)
print(document)
print("-------")
print(response)

```

QEU:FOUNDER  ： “この2つのクエリが同じであれば確実にYESが出力されるはずだが、そうでない場合・・・。それでは、プログラムを実行してみましょう。”

![imageSWC1-6-5](/2025-04-05-QEUR23_SIWC5/imageSWC1-6-5.jpg) 

D先生： “うまく予想したYESが出力されています。それでは、２つのクエリが異なるケースですが・・・。“

![imageSWC1-6-6](/2025-04-05-QEUR23_SIWC5/imageSWC1-6-6.jpg) 

QEU:FOUNDER ： “これも、うまくNOが出てきます。それでは、次に行きましょう。”

```python
# ---
### Generate
from langchain_core.output_parsers import StrOutputParser

# ---
# Test用の関数
def rag_test_generation(question):
    # ---
    # RAG_Prompt
    rag_prompt = """あなたは質問回答タスクが得意な優秀なAIアシスタントです。次の取得したコンテキスト('Context')を使用して質問に答えてください。
    コンテキストの情報が不足しているため回答できない場合には、「コンテキストが不足しているためわかりません」と回答してください。
    もし、あなたがその他の原因で適切な回答を生成できない場合は、「私にはわかりません」と言ってください。
    最大5つの文を使用し、答えは簡潔にしてください。

    Question: {question} 

    Context: {context} 

    Answer:"""
    # ---
    docs = retriever.invoke(question)
    #print(docs)
    #doc_txt = docs[1].page_content
    doc_txt = format_docs(docs)
    rag_prompt_formatted = rag_prompt.format(context=doc_txt, question=question)
    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])
    #print(generation)

    return doc_txt, generation

# ---
# Test(1)
question = "QEUシステムにおける外観検査法の特徴を教えてください。"
document, response = rag_test_generation(question)
print(document)
print("-------")
print(response)

```

QEU:FOUNDER： “一見は、普通のRAG推論ですが、ちょっとした仕掛けがあります。コンテキストの量と質が十分でないと、エラーが出てくるようになっています。それでは、外観検査についてやってみましょう。もちろん、この場合には、コンテキストは十分のはずだが・・・。”

![imageSWC1-6-7](/2025-04-05-QEUR23_SIWC5/imageSWC1-6-7.jpg) 

D先生： “おっと！参考にしたURLが表示されています。コレをうまくデータベース化すると、特定の質問に対して、どのURLが有効なのかを集計できます。 “

QEU:FOUNDER  ： “逆に言うと、長期的に使ってみて、閲覧数が少ないURLは無駄だから消すこともできますね。さて、回答の詳細を見てみましょう。”

content='QEUシステムにおける外観検査法の特徴は、Vision Transformer（VIT）とSOART3メトリックスを組み合わせた手法を使用することです。この手法により、外観検査の自動化が可能になり、特に欠品検査などの比較的簡単な自動化案件には容易に適用できるでしょう。また、Embeddingベクトルの次元数を上げることで、より高次元の情報を利用し、識別能力を向上させることができます。さらに、NSOARTCメトリックスによる変換は計算時間がかかるが、CPUやリソース制約への影響は緩いです。' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 1129, 'to-tal_tokens': 1278, 'completion_tokens': 149}, 'model_name': 'accounts/fireworks/models/llama-v3p3-70b-instruct', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None} us-age_metadata={'input_tokens': 1129, 'output_tokens': 149, 'total_tokens': 1278}

D先生： “なるほど。ただし、ちょっと惜しいかな・・・。**QEUシステムの外観検査法にはViTとSNN(Siamese Neural Net)の2種類あって、これらは全く別の技術です。**EmbeddingベクトルはSNNに関して適用されます。まあね、RAGは会話のコンテンツを使っているので、情報がまとまっていないのは仕方がないですね。“

QEU:FOUNDER ： “それでは、次は、いよいよメイン・イベントです。BRICSとグローバルサウスに関する質問をしてみましょう！”

D先生： “そもそも、ベクトル・ストアのコンテンツに、そんなもんありましたっけ？。”

QEU:FOUNDER ： “雑談のときに、この手の話題が少しあったと思います。その程度の情報量で、まともな回答ができるかどうか・・・。”

![imageSWC1-6-8](/2025-04-05-QEUR23_SIWC5/imageSWC1-6-8.jpg) 

D先生： “**「コンテキストが不足しているのでわかりません」**と・・・。あ～あ、失敗でした。 “

content='BRICSとG7のどちらがグローバルサウスに対する影響力が高いかについては、コンテキストが不足しているためわかりません。' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 1107, 'total_tokens': 1145, 'completion_tokens': 38}, 'model_name': 'ac-counts/fireworks/models/llama-v3p3-70b-instruct', 'system_fingerprint': '', 'finish_reason': 'stop', age_metadata={'input_tokens': 1107, 'output_tokens': 38, 'total_tokens': 1145}

QEU:FOUNDER ： “いや、これは成功でしょう。ちゃんと、LLMは情報量の不足も検出してくれるんですね。”

D先生： “これで、「structured_output」が利用できるんですかねえ？ “

QEU:FOUNDER ： “なんともいえんなあ・・・。ただし、「コンテキストが不足しているのでわかりません」というコメントが安定して出るんだったら、べつにどうでもいいですよ。これで判別ができれば、コンテキスト不足を検出したら、自動的にインターネット検索に切り替えればいいだけですね。”

D先生： “次は、インターネット検索ですか・・・。”

QEU:FOUNDER ： “まあ・・・。おいおい考えましょう。次も、さらに「閑話休題」です。”



## ～ まとめ ～

### ・・・ とうとう、世界大恐慌が来たのか！？ ・・・

D先生：“とうとう来ました！ここまでやるとは、さすがに変人FOUNDERでも思わなかったですよね？”

![imageSWC1-6-9](/2025-04-05-QEUR23_SIWC5/imageSWC1-6-9.jpg) 

QEU:FOUNDER ： “T大統領が、**「おかしなことをやる」**とは思ってましたけど・・・。まさか、これほどとは・・・。”

![imageSWC1-6-10](/2025-04-05-QEUR23_SIWC5/imageSWC1-6-10.jpg) 

QEU:FOUNDER ： “ただしねえ・・・。これらの政策パッケージのうち、**「彼が本当にやりかったこと」はどれだけあったのだろうか**・・・。さて、今回の事象の根本原因を映す鏡は**「金の価格」**です。それでは、ちょっとGROK様に質問（↑）をしてみましょう。”

### 1. 金のドルベース価格上昇の背景

金のドルベース価格（例えば、1トロイオンスあたりのドル価格）は、2025年4月5日時点で過去最高水準に達していると仮定します（参考として、2024年9月には2,600ドルを超えたとの情報があります）。この価格上昇は、金そのものの価値（需要と供給）と、ドル自体の価値（購買力や信頼性）の双方に影響を受けています。

### 2. 要因の分解と分析

### (1) 金の価値の上昇（需要増加・供給制約）

金の価値が上昇する要因は、主に以下のようなものがあります：

- 地政学的リスクの増大: 戦争や政情不安が続くと、投資家は「安全資産」として金を求める傾向が強まります。2020年代のウクライナ問題や中東情勢の緊迫化が例です。
- 中央銀行の金購入: 新興国の中央銀行（特に中国やインド）が外貨準備として金を増やしており、これが需給を逼迫させています。2024年のデータでは、中央銀行の約3割が今後12カ月で金を増やす意向を示しています。
- インフレヘッジ: 世界的なインフレ圧力が高まる中、金は価値保存手段として注目されています。
- 供給の限界: 金の採掘量は地球上の埋蔵量に限りがあり、新規供給が減少傾向にあるため、希少性が価格を押し上げます。

### 貢献度の推定:

過去数年のトレンドを基にすると、金価格の上昇のうち、需要増加（特に中央銀行や投資家の買い）が約40〜60%を占めると考えられます。例えば、2020年から2024年にかけて金価格が約2倍（1,300ドルから2,600ドル程度）に上昇した背景には、中央銀行の購入や地政学リスクが大きく寄与しています。
 
D先生：“当たり前ですが、金の価格上昇の原因のうち、多くの貢献した要因は「金の需要が高い」ことです。しかし、このGROK氏によると、残りの60-40％は「ドルの価値減少」にも起因するようです。コメントは続きます。”

### (2) ドルの価値の低下（購買力低下・信頼性低下）

ドルの価値が低下する要因は以下の通りです：

- 米国の金融政策: 2020年以降、FRB（連邦準備制度）はコロナ禍対応として大規模な量的緩和を実施し、通貨供給量が急増しました。これにより、ドルの購買力が希薄化しています。
- ドル離れの動き: 中国やロシアなど一部の国がドル依存を減らし、金や他通貨での取引を増やす「デドルの兆候」が見られます。
- インフレと金利: 米国のインフレ率が上昇する一方、低金利政策が続くと、ドルの実質価値が低下します。
- 為替変動: ドルがユーロや円に対して弱含む場合、金価格（ドル建て）が相対的に上昇します。

### 貢献度の推定:

ドルの価値低下が金価格上昇に与える影響は、歴史的に逆相関関係（金とドルの相関係数は約-0.7〜-0.9）が強いことを踏まえると、全体の30〜50%程度と推定されます。例えば、2008年の金融危機後や2020年のコロナ禍で、ドル供給量の増加が金価格を押し上げた事例があります。
 
QEU:FOUNDER ： “GROK氏が定量化してくれました。眺めてみましょう。ドン！！”

### 3. 定量化の試み

具体的な数値例を仮定して分解してみます（2025年4月時点の最新データがないため、2024年までのトレンドを基にした推測です）：
仮定: 金価格が2020年の1,500ドルから2025年に3,000ドルに上昇したとする（上昇額：1,500ドル）。

- 金の価値上昇の寄与: 中央銀行の需要増や地政学リスクで約800ドル（全体の53%）。
- ドルの価値低下の寄与: 通貨供給量の拡大やインフレで約700ドル（全体の47%）。

この割合は状況によって変動します。例えば、ドルインデックス（DXY）が急落すればドルの寄与が60%を超える可能性もありますし、地政学リスクがさらに高まれば金の寄与が70%近くになる場合もあります。
 
C部長： “うわあ・・・。GROK氏、分析とイマジネーションの能力もツヨツヨ・・・。”

D先生： “前に、誰かのコメントを見たことがあるなあ。誰だったっけ・・・？”

![imageSWC1-6-11](/2025-04-05-QEUR23_SIWC5/imageSWC1-6-11.jpg) 

QEU:FOUNDER ： “ちょっと**T大統領に同情する**ワタクシ・・・。”

D先生：“当のJ国はどうなっていくのでしょうか？”

![imageSWC1-6-12](/2025-04-05-QEUR23_SIWC5/imageSWC1-6-12.jpg) 

QEU:FOUNDER ： “この図(↓)を見れば、大体の状況は読めますよ。ホラ・・・。”

![imageSWC1-6-13](/2025-04-05-QEUR23_SIWC5/imageSWC1-6-13.jpg) 

C部長： “そのココロは・・・？”

QEU:FOUNDER ： “最近のSK国とJ国輸出量をみてみ？少しだけJ国の輸出量が上でしょう？もし、今回の関税問題で**自動車の輸出量が激減**したらどうなる？自動車一本足打法のJ国が・・・。”

C部長： “ギャー！！”

D先生： “**J国の輸出量がSK国に負けます。SK国の2倍の人口を持つJ国が・・・。**一体、どうなるのでしょうね？”

![imageSWC1-6-14](/2025-04-05-QEUR23_SIWC5/imageSWC1-6-14.jpg) 

QEU:FOUNDER ： “わかんないです。ただし、もう少し経てばわかるようになります。**非常にクレバーなV国が仕切った今度の会議(↑)に、なんとE国が参加しません。**これがポイントです。あと、ASEANの多くの国々がC国とつながったんです。厳密にいうと、**「もうすぐつながる」**か・・・。”

![imageSWC1-6-15](/2025-04-05-QEUR23_SIWC5/imageSWC1-6-15.jpg) 

D先生：“20世紀を象徴した、**「海洋国家群がブイブイ言わせた時代」が近い将来に変わってくる**んでしょうかね？”

![imageSWC1-6-16](/2025-04-05-QEUR23_SIWC5/imageSWC1-6-16.jpg) 

C部長： “あれ？いまは、A国にある、この手の会社群（↑）にとっては、今回の状況は本来**「ボーナス・タイム」**のはず・・・。なんで、ここまで（株価が）下がるんだ！？”

QEU:FOUNDER ： “そもそも、自動車っていう商売は**「海洋国家の産物」**だと思うんです。自動車は、**「大型貨物船で輸出するには、ちょうどいい大きさの高付加価値品」**です。一方、大陸国家は、自動車に対して大きく依存しないんです。**ちゃんとした鉄道があれば**ね・・・。A国は、本来は大陸国家であるにも関わらず、自動車に依存し過ぎて鉄道建設に失敗した。これが本当にあるべき国造り（**大陸国家であり、かつ海洋国家である超大国**）の達成に著しく失敗した。その延長に、T大統領が活躍する現在がある。”
