---
title: QEUR23_AENC1 - 便利だよConditional V-AUTOENCODER
date: 2025-06-27
tags: ["QEUシステム", "メトリックス", "Python言語", "Unsloth", "LLM", "データセット", "BONSAI", "LangGraph"]
excerpt: あたらしいLLMの学習体系を確立する
---

## QEUR23_AENC1 - 便利だよConditional V-AUTOENCODER

### ～ VAEは、外観検査自動機の（ほぼ）最終形 ～

C部長： “いきなりですが、すいません。ちょっと、この製品（↓）で検査を自動化したいんです。”

![imageANC0-2-1](/2025-06-27-QEUR23_AENC1/imageANC0-2-1.jpg) 

QEU:FOUNDER ： “もちろん！やれますよ。コネクタの端子検査は重要だからね。”

C部長： “それ以外にも、**ワイヤを束ねたハーネス部の幹や枝**も検査したいんです。”

![imageANC0-2-2](/2025-06-27-QEUR23_AENC1/imageANC0-2-2.jpg) 

D先生： “困ったなあ・・・。コネクタであれば、RTメトリックスで前処理をすれば検査ができるのに・・・。”

C部長： “はぁ・・・、簡単にできると思っていました。。”

**（外観検査の難しさ）**

- 外観検査は、被検査物が検査治具などで固定をしているのが望ましい

- 組立品の場合、部品間違いがありうる（部品としては正しいが、組み立ててはいけない）

QEU:FOUNDER ： “じゃあ、今回は、**Conditional Autoencoder**をやりましょう。”

D先生： “あれ？これは、前回のAutoencoderとは違うんですか？”

![imageANC0-2-3](/2025-06-27-QEUR23_AENC1/imageANC0-2-3.jpg) 

QEU:FOUNDER ： “構造は、基本的にAutoencoderなのだが、ユーザーが同時にラベル情報をモデルに入力できます。だから、ユーザーが「これは、コネクタAです」と入力したのに、画像がコネクタBであればエラーが起きるんです。”

D先生： “それは面白い技術だ！？”

QEU:FOUNDER ： “それでは、とても簡単なプログラムを組んできました。”

```python
# ----
import numpy as np
import matplotlib.pyplot as plt
from skimage import draw
from skimage.transform import rotate
import tensorflow as tf
from tensorflow.keras import layers, Model
from tensorflow.keras.utils import to_categorical

# --- GPUメモリ設定（必要に応じて） ---
physical_devices = tf.config.list_physical_devices('GPU')
if physical_devices:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)

# --- 画像生成関数 ---
def generate_image(size=27, shape_type='solid_circle'):
    img = np.zeros((size, size), dtype=np.float32)
    center = size // 2
    variation = size // 9
    center_x = center + np.random.randint(-variation, variation + 1)
    center_y = center + np.random.randint(-variation, variation + 1)
    min_radius = size // 6
    max_radius = size // 3
    radius = np.random.randint(min_radius, max_radius + 1)
    angle = np.random.uniform(-20, 20)

    if shape_type in ['solid_circle', 'hollow_circle']:
        rr, cc = draw.disk((center_y, center_x), radius, shape=img.shape)
        img[rr, cc] = 1.0
        if shape_type == 'hollow_circle':
            inner_radius = np.random.randint(2, radius - 1)
            rr, cc = draw.disk((center_y, center_x), inner_radius, shape=img.shape)
            img[rr, cc] = 0.0
    else:
        half_size = np.random.randint(min_radius, max_radius + 1)
        start_x = center_x - half_size
        end_x = center_x + half_size
        start_y = center_y - half_size
        end_y = center_y + half_size
        rr, cc = draw.rectangle((start_y, start_x), (end_y, end_x), shape=img.shape)
        img[rr, cc] = 1.0
        if shape_type == 'hollow_rectangle':
            min_length = int(min(abs(start_x - end_x), abs(start_y - end_y)) * 0.5)
            hollow_thickness = np.random.randint(1, max(2, min_length - 2))
            inner_start_x = start_x + hollow_thickness
            inner_end_x = end_x - hollow_thickness
            inner_start_y = start_y + hollow_thickness
            inner_end_y = end_y - hollow_thickness
            if inner_end_x > inner_start_x and inner_end_y > inner_start_y:
                rr, cc = draw.rectangle(
                    (inner_start_y, inner_start_x),
                    (inner_end_y, inner_end_x),
                    shape=img.shape
                )
                img[rr, cc] = 0.0

    img = rotate(img, angle, resize=False, order=0, mode='constant', cval=0)
    img = (img > 0.5).astype(np.float32)
    return img

# --- データセット生成 ---
def generate_dataset(num_per_class, size=27):
    shape_types = ['solid_circle', 'hollow_circle', 'solid_rectangle', 'hollow_rectangle']
    X, y = [], []
    for label, shape in enumerate(shape_types):
        for _ in range(num_per_class):
            img = generate_image(size=size, shape_type=shape)
            X.append(img)
            y.append(label)
    X = np.array(X).reshape(-1, size, size, 1)
    y = np.array(y)
    return X, y

# --- データ生成 ---
num_train_per_class = 1000
num_test_per_class = 100
input_shape = (27, 27, 1)
num_classes = 4

X_train, y_train = generate_dataset(num_train_per_class, size=27)
X_test, y_test = generate_dataset(num_test_per_class, size=27)
y_train_onehot = to_categorical(y_train, num_classes)
y_test_onehot = to_categorical(y_test, num_classes)

# -------------------------------
# 例画像の表示
# -------------------------------
shape_names = ['Solid Circle', 'Hollow Circle', 'Solid Rect', 'Hollow Rect']
plt.figure(figsize=(12, 6))
for k in range(4):
    idx = np.where(y_train == k)[0][0]
    plt.subplot(2, 4, k+1)
    plt.imshow(X_train[idx].reshape(27, 27), cmap='gray')
    plt.title(f'Train: {shape_names[k]}')
    plt.axis('off')
    idx = np.where(y_test == k)[0][0]
    plt.subplot(2, 4, k+5)
    plt.imshow(X_test[idx].reshape(27, 27), cmap='gray')
    plt.title(f'Test: {shape_names[k]}')
    plt.axis('off')
plt.suptitle('Generated Images (27x27 pixels)', fontsize=16)
plt.tight_layout()
plt.show()
```

QEU:FOUNDER ： “今回は、図形の種類をわざと増やしています。”

![imageANC0-2-4](/2025-06-27-QEUR23_AENC1/imageANC0-2-4.jpg) 

D先生： “なるほど。そして、それぞれの図形に揺らぎがあるんですね。どれぐらいの画像を準備したんですか？”

QEU:FOUNDER ： “各1000件です”

D先生： “WOW! スゴイ数量を準備しましたね。”

QEU:FOUNDER ： “モデルが、前回よりも複雑になりました。それでは、つづきに行きましょう。”

```python
# -------------------------------
# CONDITIONAL VARIATIONAL AUTOENCODERによる、検査画像の学習
# -------------------------------
# --- Conditional VAEのサブクラス定義 ---
class Sampling(layers.Layer):
    def call(self, inputs):
        z_mean, z_log_var = inputs
        epsilon = tf.random.normal(shape=tf.shape(z_mean))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon

class ConditionalVAE(Model):
    def __init__(self, input_shape, num_classes, latent_dim=128):
        super().__init__()
        self.latent_dim = latent_dim
        self.num_classes = num_classes

        # --- Encoder ---
        self.encoder_conv = tf.keras.Sequential([
            layers.Conv2D(32, 3, strides=2, padding='same', activation='relu'),
            layers.BatchNormalization(),
            layers.Conv2D(64, 3, strides=2, padding='same', activation='relu'),
            layers.BatchNormalization(),
            layers.Conv2D(128, 3, strides=2, padding='same', activation='relu'),
            layers.BatchNormalization(),
            layers.Flatten(),
        ])
        self.dense_mean = layers.Dense(latent_dim)
        self.dense_log_var = layers.Dense(latent_dim)
        self.sampling = Sampling()

        # --- Decoder ---
        self.decoder_dense = layers.Dense(7 * 7 * 128, activation='relu')
        self.decoder_reshape = layers.Reshape((7, 7, 128))
        self.decoder_conv = tf.keras.Sequential([
            layers.Conv2D(128, 3, padding='same', activation='relu'),
            layers.BatchNormalization(),
            layers.UpSampling2D(),
            layers.Conv2D(64, 3, padding='same', activation='relu'),
            layers.BatchNormalization(),
            layers.UpSampling2D(),
            layers.Cropping2D(((0, 1), (0, 1))),
            layers.Conv2D(32, 3, padding='same', activation='relu'),
            layers.BatchNormalization(),
            layers.Conv2D(1, 3, padding='same', activation='sigmoid')
        ])

    def encode(self, x, y):
        x_encoded = self.encoder_conv(x)
        x_concat = tf.concat([x_encoded, y], axis=1)
        z_mean = self.dense_mean(x_concat)
        z_log_var = self.dense_log_var(x_concat)
        z = self.sampling((z_mean, z_log_var))
        return z_mean, z_log_var, z

    def decode(self, z, y):
        z_cond = tf.concat([z, y], axis=1)
        x = self.decoder_dense(z_cond)
        x = self.decoder_reshape(x)
        x = self.decoder_conv(x)
        return x

    def call(self, inputs):
        x, y = inputs
        z_mean, z_log_var, z = self.encode(x, y)
        x_recon = self.decode(z, y)
        self.z_mean = z_mean
        self.z_log_var = z_log_var
        return x_recon

    def train_step(self, data):
        (x, y), x_true = data
        with tf.GradientTape() as tape:
            x_pred = self((x, y), training=True)
            rec_loss = tf.reduce_mean(tf.reduce_sum(tf.square(x_true - x_pred), axis=[1,2,3]))
            kl_loss = -0.5 * tf.reduce_mean(tf.reduce_sum(1 + self.z_log_var - tf.square(self.z_mean) - tf.exp(self.z_log_var), axis=1))
            total_loss = rec_loss + kl_loss
        grads = tape.gradient(total_loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))
        return {"loss": total_loss, "reconstruction_loss": rec_loss, "kl_loss": kl_loss}

    def test_step(self, data):
        (x, y), x_true = data
        x_pred = self((x, y), training=False)
        rec_loss = tf.reduce_mean(tf.reduce_sum(tf.square(x_true - x_pred), axis=[1,2,3]))
        kl_loss = -0.5 * tf.reduce_mean(tf.reduce_sum(1 + self.z_log_var - tf.square(self.z_mean) - tf.exp(self.z_log_var), axis=1))
        total_loss = rec_loss + kl_loss
        return {"loss": total_loss, "reconstruction_loss": rec_loss, "kl_loss": kl_loss}

# --- メイン処理 ---
# パラメタ設定
latent_dim = 128

# --- モデル構築 ---
cvae = ConditionalVAE(input_shape, num_classes, latent_dim)
cvae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))

# --- 学習 ---
history = cvae.fit(
    (X_train, y_train_onehot), X_train,
    epochs=50,
    batch_size=64,
    validation_data=((X_test, y_test_onehot), X_test),
    verbose=2,
)

# --- 損失曲線のプロット ---
plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Conditional VAE Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

```

QEU:FOUNDER ： “こんな良いモデルは、小生だけでは半年勉強しても実現できません。**Vibe Codingに感謝しかない**わね。学習結果に行きましょう。”

![imageANC0-2-5](/2025-06-27-QEUR23_AENC1/imageANC0-2-5.jpg) 

D先生： “かなりうまく収束しています。EPOCH数が50は、少なすぎたのでは？”

QEU:FOUNDER ： “PCで動かしたので、(計算が)遅いんです。小生がお金持ちで、バンバンGPUを使えば、100EPOCHぐらいは、すぐにできます。それでは、このCVAEモデルの評価に入りましょう。まずは、CVAEモデルが、どの程度正確な画像を出力するのかを見てみましょう。この場合には、ユーザー指示と画像の種類は一致しています。指示と画像が「中実円形」であるとかね・・・。”

![imageANC0-2-6](/2025-06-27-QEUR23_AENC1/imageANC0-2-6.jpg) 

D先生： “おっと！このモデルは結構よい精度で画像を出力しています。やはり、前回の事例において出力画像が汚かった原因はモデルが単純すぎたからのようです。”

QEU:FOUNDER ： “それでは、さらに評価が続きます。これから、指示と検査画像に不一致が起こります。”

```python
# -------------------------------
# (評価NO1) 再構成画像のテスト - 4種類の図形を個別に表示
# -------------------------------
plt.figure(figsize=(15, 12))
shape_names = ['Solid Circle', 'Hollow Circle', 'Solid Rect', 'Hollow Rect']

for class_idx in range(4):
    # 各クラスからランダムにサンプルを選択
    class_samples = np.where(y_test == class_idx)[0]
    sample_idx = np.random.choice(class_samples)
    
    test_input = X_test[sample_idx:sample_idx+1]
    true_label = y_test_onehot[sample_idx:sample_idx+1]
    
    # 再構成画像の生成
    reconstructed = model.predict([test_input, true_label], verbose=0)
    
    # 差の絶対値を計算
    diff = np.abs(test_input[0] - reconstructed[0])
    
    # オリジナル画像
    plt.subplot(4, 3, class_idx*3+1)
    plt.imshow(test_input[0].squeeze(), cmap='gray', vmin=0, vmax=1)
    plt.title(f'Original: {shape_names[class_idx]}')
    plt.axis('off')
    
    # 再構成画像
    plt.subplot(4, 3, class_idx*3+2)
    plt.imshow(reconstructed[0].squeeze(), cmap='gray', vmin=0, vmax=1)
    plt.title('Reconstructed')
    plt.axis('off')
    
    # 差の画像 (絶対値)
    plt.subplot(4, 3, class_idx*3+3)
    plt.imshow(diff.squeeze(), cmap='hot', vmin=0, vmax=1)
    plt.title('Difference')
    plt.axis('off')
    plt.colorbar(fraction=0.046, pad=0.04)

plt.tight_layout()
plt.suptitle('Reconstruction Results with Difference Maps', fontsize=16, y=1.02)
plt.show()

```

QEU:FOUNDER ： “我々は、ユーザーが指定した画像を生成する手法については、STABLE DIFFUSIONしか知りませんでした。こんな方法があるとは思わなかったですね。”

![imageANC0-2-7](/2025-06-27-QEUR23_AENC1/imageANC0-2-7.jpg) 

D先生： “イメージした通りの図形が生成されています。これはいい・・・。”

QEU:FOUNDER ： “最後に差異の画像を見てみましょう。”

![imageANC0-2-8](/2025-06-27-QEUR23_AENC1/imageANC0-2-8.jpg) 

D先生： “すばらしい・・・。それにしてもmseという指標では、あまり良い結果が出ていませんね。うまく識別ができていません。結果をまとめてもらえませんか？”

![imageANC0-2-9](/2025-06-27-QEUR23_AENC1/imageANC0-2-9.jpg) 

D先生： “それにしても、我々が差異画像を見れば、頭脳で判別がほぼ確実にできます。やはり、単純なメトリックスmseを判別につかうのは良くないですね。”

QEU:FOUNDER ： “一番良い判別方法は、ちょっと古典的だがサポートベクトルマシン(SVM)じゃないでしょうか。学習データの件数が少なくてもいいですからね。”

![imageANC0-2-10](/2025-06-27-QEUR23_AENC1/imageANC0-2-10.jpg) 

C部長 ： “これならば、**自分で検査用のロボットを作ったほうがよさそう**だ・・・。このプログラムをVibe Codingで改造していけばいいんでしょ？”

QEU:FOUNDER ： “そうです。このシステムの改造は簡単だと思います。ロボットと機械学習に強い若い新入社員に頼めば、半年ぐらいでプロトタイプが作れると思います。”

D先生： “データ取りの量と、最終精度が気になるところです。楽しみ楽しみ・・・。”



## ～ まとめ ～


### ・・・ 前回の続きです ・・・

QEU:FOUNDER ： “ぶっちゃけ、そうじゃないの？悪いのは、オッサンどもで・・・。”

![imageANC0-2-11](/2025-06-27-QEUR23_AENC1/imageANC0-2-11.jpg) 

D先生： “支持政党の構成表を見ると、「何が起こった」か、だいたいのところは分かります。あそこの票があそこに流れたわけね・・・。”

[![MOVIE1](http://img.youtube.com/vi/NdHsBv6L9OI/0.jpg)](http://www.youtube.com/watch?v=NdHsBv6L9OI "平成政治史全部解説！")

QEU:FOUNDER ： “平成の時代は、**大衆が頼りにしていたメディアの政治に対する批判能力が大幅に落ちた時期**なので、このころに働き始めた「当時若者たち」は、あまり政治の変化を理解していないんです。そこに、いきなりSNSで雑多な情報が流れ込んで、人々が無批判に受け入れたんです。”

![imageANC0-2-12](/2025-06-27-QEUR23_AENC1/imageANC0-2-12.jpg) 

D先生： “80以上になると、政党の特徴、ましては近年の変化がわからない。時間の概念を喪失したのか、角さんの魂が、まだ**あそこの本流だと信じ込む**。”

![imageANC0-2-13](/2025-06-27-QEUR23_AENC1/imageANC0-2-13.jpg) 

C部長 : “70になると、自分がやっていたことが、如何に現在の壊滅的な状況に影響を与えて来たかを自覚しています。”

- **古い(平成)おっさん（＠QCサークル講話）；「従業員の皆さんにはテレビを見てください。皆が同じように考えてください。」**

- **古い(平成)オッサン（＠車中、N社検査不正について）： 「“検査不正”っていうのはなァ、（組織外に不正を）漏らしたヤツが悪いんだよ・・・」**
 
- **古い(平成)オッサン（海外工場のあいさつにて、なんと201X年のセリフじゃ！）：「私の使命はこの会社で終身雇用制を実現することにある・・・。」**

D先生 ： “そういう意味では、彼らには「時間の概念」があるんだよね。”

![imageANC0-2-14](/2025-06-27-QEUR23_AENC1/imageANC0-2-14.jpg) 

QEU:FOUNDER ： “まあね・・・。自分の息子娘世代を自分の立場のために地獄に落として現在に至るんだから、そりゃ、**いやでも毎日自覚するわな**。だけど、その分だけ精神バランスを取るためにおかしいことが起きる。**2010年代の保守ムーブ**はここらへんでしょ？”

![imageANC0-2-15](/2025-06-27-QEUR23_AENC1/imageANC0-2-15.jpg) 

QEU:FOUNDER ： “ただし、それでも人口構成比を考えると、少なくとも今後はマイナーになるんですよ。”

C部長 : “以上の議論によると、消去法的、必然的に50-60が、問題の軸になってきます。”

![imageANC0-2-16](/2025-06-27-QEUR23_AENC1/imageANC0-2-16.jpg) 

C部長 : “この図に異議あり！あのグループは、**加速主義**なんですかねえ？”

![imageANC0-2-17](/2025-06-27-QEUR23_AENC1/imageANC0-2-17.jpg) 

QEU:FOUNDER ： “この図（↑）を参考にしているのだが・・・。特に、最近人気の2つのグループは、**「何かにとりつかれて」加速している**ようにしか思えません。”

![imageANC0-2-18](/2025-06-27-QEUR23_AENC1/imageANC0-2-18.jpg) 

QEU:FOUNDER ： “実は、自〇党って、あくまで「条件付き」なんだけど、まだマシな方だと思うんです。”
