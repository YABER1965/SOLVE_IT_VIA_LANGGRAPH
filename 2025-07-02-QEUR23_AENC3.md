---
title: QEUR23_AENC3 - より鮮明な画像を使いVATモデルを端子検査に適用してみる
date: 2025-07-02
tags: ["QEUシステム", "メトリックス", "Python言語", "Unsloth", "LLM", "データセット", "BONSAI", "LangGraph"]
excerpt: あたらしいLLMの学習体系を確立する
---

## QEUR23_AENC3 - より鮮明な画像を使いVATモデルを端子検査に適用してみる

## ～ べつに、感動もない。でも、いいよ・・・ ～

QEU:FOUNDER（設定年齢65歳） ： “前回は、NSOARTメトリックスのSTEP2で生成した粗っぽい合成画像(36x24)を使って、 VAEによる判別をしてみました。まあ、そこそこの出来ともいえるが、やはり一番の失敗は、「画像」だね・・・（笑）。”

![imageANC0-4-1](/2025-07-02-QEUR23_AENC3/imageANC0-4-1.jpg) 

D先生（設定年齢65歳）： “それを言っては元も子もないです。じゃあ、STEP1の画像に切換し、さらにモデルもレベルアップしましょう。”

![imageANC0-4-2](/2025-07-02-QEUR23_AENC3/imageANC0-4-2.jpg) 

D先生： “今回は、当然のこととして、モデルへの入力画像が大きくなったことによってGPUを使うことが不可避だったでしょう？”

![imageANC0-4-3](/2025-07-02-QEUR23_AENC3/imageANC0-4-3.jpg) 

QEU:FOUNDER  ： “はっきり言って、T4程度のGPUでも意外ときついですよ。**「VAEモデルは、GPUメモリを大量消費する」**ことを思い知りました。そういえば、AUTOENCODERって、Stable Diffusionの基礎技術です。あのモデルの学習と同じことをやっているんだから、T4程度のGPUではきついですね。さあ、プログラムを見てみましょう。”

```python
# ---
# 必要なライブラリ
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, Model, callbacks
from datasets import load_dataset
import matplotlib.pyplot as plt
import logging
import os

# ロギング設定
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # TF警告抑制
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# GPUメモリ設定
physical_devices = tf.config.list_physical_devices('GPU')
if physical_devices:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)

# --- データセット読み込み・前処理 ---
def preprocess_image(image):
    """画像前処理関数"""
    try:
        img = image.convert('L').resize((128, 128))
        img = np.array(img, dtype=np.float32) / 255.0
        return img.reshape(128, 128, 1)
    except Exception as e:
        logger.error(f"Error processing image: {e}")
        return None

def load_hf_dataset(repo_id, max_samples=700):
    """データセット読み込み関数"""
    dataset = load_dataset(repo_id)
    train_data = dataset['train'].select(range(min(max_samples, len(dataset['train']))))
    test_data = dataset['test'].select(range(min(max_samples, len(dataset['test']))))

    # 画像前処理
    X_train = np.array([preprocess_image(img) for img in train_data['image'] if prepro-cess_image(img) is not None])
    X_test = np.array([preprocess_image(img) for img in test_data['image'] if preprocess_image(img) is not None])
    
    logger.info(f"Dataset loaded: Train {len(X_train)} samples, Test {len(X_test)} samples")
    return X_train, X_test

# --- メイン処理 ---
# Huggingfaceデータセット読み込み
repo_id = "YxBxRyXJx/VAEimages_optimized"
logger.info("Loading dataset...")
X_train, X_test = load_hf_dataset(repo_id, max_samples=700)
logger.info(f"Train shape: {X_train.shape}, Test shape: {X_test.shape}")


# --- 画像表示関数 ---
def display_sample_images(images, title, num_images=4):
    plt.figure(figsize=(10, 5))
    for i in range(min(num_images, len(images))):
        plt.subplot(1, num_images, i+1)
        plt.imshow(images[i].squeeze(), cmap='gray')
        plt.title(f"Sample {i+1}")
        plt.axis('off')
    plt.suptitle(title)
    plt.tight_layout()
    plt.savefig(f"{title.replace(' ', '_')}.png")
    plt.show()

# サンプル画像表示
display_sample_images(X_train, "Training Samples")

```

QEU:FOUNDER  ： “話のついでに白状すると、8GB程度のGPUで生画像(382, 214)では、学習できないんです。それで、**サイズを縮小(128, 128)**をしました。”

![imageANC0-4-4](/2025-07-02-QEUR23_AENC3/imageANC0-4-4.jpg) 

D先生： “・・・なるほどね。それじゃあ、例えば外観検査事例として、**「大きなパネル部品にある小さな黒点を見つける」**ようなスゴイ芸当は、もうコスト的に難しくなりますね。”

QEU:FOUNDER  ： “そんな大それた学習をするにはA100、もしくはH100位のパワフルなGPUがいるんじゃないですか？ただし、モデルの学習が終わり、画像の再構成（異常の予測）だけの作業ではそれほど強力なリソースが必要じゃないかもしれません。それでは、つづきにいきましょう。次にVAEの学習に入ります。”

```python
# --- VAE用サンプリングレイヤー ---
class Sampling(layers.Layer):
    def call(self, inputs):
        z_mean, z_log_var = inputs
        epsilon = tf.random.normal(shape=tf.shape(z_mean))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon

# --- 修正・強化されたVAEモデル定義 ---
class EnhancedVAE(Model):
    def __init__(self, input_shape, latent_dim=64):
        super().__init__()
        self.latent_dim = latent_dim
        self.total_loss_tracker = tf.keras.metrics.Mean(name="total_loss")
        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(name="reconstruction_loss")
        self.kl_loss_tracker = tf.keras.metrics.Mean(name="kl_loss")
        
        # エンコーダ - 128x128を8x8にダウンサンプリング
        self.encoder = tf.keras.Sequential([
            layers.InputLayer(input_shape=input_shape),
            layers.Conv2D(32, 3, strides=2, padding='same', activation='relu'),
            layers.BatchNormalization(),
            layers.Conv2D(64, 3, strides=2, padding='same', activation='relu'),
            layers.BatchNormalization(),
            layers.Conv2D(128, 3, strides=2, padding='same', activation='relu'),
            layers.BatchNormalization(),
            layers.Conv2D(256, 3, strides=2, padding='same', activation='relu'),
            layers.BatchNormalization(),
            layers.Flatten(),
            layers.Dense(latent_dim * 2),
        ], name="encoder")
        
        self.sampling = Sampling()
        
        # デコーダ - 8x8を128x128にアップサンプリング
        self.decoder = tf.keras.Sequential([
            layers.InputLayer(input_shape=(latent_dim,)),
            layers.Dense(8*8*256, activation='relu'),
            layers.Reshape((8, 8, 256)),
            layers.Conv2DTranspose(256, 3, strides=2, padding='same', activation='relu'),
            layers.BatchNormalization(),
            layers.Conv2DTranspose(128, 3, strides=2, padding='same', activation='relu'),
            layers.BatchNormalization(),
            layers.Conv2DTranspose(64, 3, strides=2, padding='same', activation='relu'),
            layers.BatchNormalization(),
            layers.Conv2DTranspose(32, 3, strides=2, padding='same', activation='relu'),
            layers.Conv2D(1, 3, padding='same', activation='sigmoid'),
        ], name="decoder")

    def encode(self, x):
        h = self.encoder(x)
        z_mean, z_log_var = tf.split(h, num_or_size_splits=2, axis=1)
        return z_mean, z_log_var, self.sampling((z_mean, z_log_var))

    def decode(self, z):
        return self.decoder(z)

    def call(self, inputs):
        _, _, z = self.encode(inputs)
        return self.decode(z)
    
    @property
    def metrics(self):
        return [
            self.total_loss_tracker,
            self.reconstruction_loss_tracker,
            self.kl_loss_tracker,
        ]

    def train_step(self, data):
        with tf.GradientTape() as tape:
            z_mean, z_log_var, z = self.encode(data)
            reconstruction = self.decode(z)
            
            # 再構成損失 (平均二乗誤差)
            rec_loss = tf.reduce_mean(
                tf.reduce_sum(
                    tf.square(data - reconstruction),
                    axis=(1, 2)
                )
            )
            
            # KLダイバージェンス損失
            kl_loss = -0.5 * tf.reduce_mean(
                tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)
            )
            
            # 動的なKL損失の重み付け (TensorFlow演算を使用)
            kl_weight = tf.minimum(1.0, tf.cast(self.optimizer.iterations, tf.float32) / 10000.0)
            total_loss = rec_loss + kl_weight * kl_loss
        
        # 勾配計算と更新
        grads = tape.gradient(total_loss, self.trainable_weights)
        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))
        
        # メトリクスの更新
        self.total_loss_tracker.update_state(total_loss)
        self.reconstruction_loss_tracker.update_state(rec_loss)
        self.kl_loss_tracker.update_state(kl_loss)
        
        return {
            "loss": self.total_loss_tracker.result(),
            "reconstruction_loss": self.reconstruction_loss_tracker.result(),
            "kl_loss": self.kl_loss_tracker.result(),
        }
    
    def test_step(self, data):
        # バリデーション用の損失計算
        z_mean, z_log_var, z = self.encode(data)
        reconstruction = self.decode(z)
        
        rec_loss = tf.reduce_mean(
            tf.reduce_sum(
                tf.square(data - reconstruction),
                axis=(1, 2)
            )
        )
        
        kl_loss = -0.5 * tf.reduce_mean(
            tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)
        )
        
        # 動的なKL損失の重み付け (TensorFlow演算を使用)
        kl_weight = tf.minimum(1.0, tf.cast(self.optimizer.iterations, tf.float32) / 10000.0)
        total_loss = rec_loss + kl_weight * kl_loss
        
        return {
            "loss": total_loss,
            "reconstruction_loss": rec_loss,
            "kl_loss": kl_loss,
        }

# --- VAEモデル構築 ---
vae = EnhancedVAE(input_shape=(128, 128, 1), latent_dim=64)

# 学習率スケジューリング
initial_learning_rate = 1e-3
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate,
    decay_steps=1000,
    decay_rate=0.95
)
optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)
vae.compile(optimizer=optimizer)

# コールバック設定
early_stopping = callbacks.EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True,
    min_delta=0.001
)

checkpoint = callbacks.ModelCheckpoint(
    'best_vae_model.keras',
    save_best_only=True,
    monitor='val_loss'
)

# 学習
logger.info("Training Enhanced VAE...")
history = vae.fit(
    X_train,
    epochs=50,
    batch_size=16,
    validation_data=(X_test,),
    callbacks=[early_stopping, checkpoint],
    verbose=1
)

# 学習履歴のプロット
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('VAE Training History')
plt.xlabel('Epoch')
plt.ylabel('Total Loss')
plt.legend()
plt.grid(True)

plt.subplot(1, 2, 2)
plt.plot(history.history['reconstruction_loss'], label='Training Rec Loss')
plt.plot(history.history['val_reconstruction_loss'], label='Validation Rec Loss')
plt.plot(history.history['kl_loss'], label='Training KL Loss')
plt.plot(history.history['val_kl_loss'], label='Validation KL Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss Components')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.savefig('vae_training_history.png')
plt.show()

```

D先生： “ここでも、学習曲線によると、前回と同様に学習が急速に進んでいますね。”

![imageANC0-4-5](/2025-07-02-QEUR23_AENC3/imageANC0-4-5.jpg) 

QEU:FOUNDER  ： “もちろん、全ての画像がほぼ同じです。**ほんの数％の欠陥が含まれているかどうか**ですからね。”

```python
# 異常検出機能（差異画像を活用した改良版）
def detect_anomalies_with_difference(model, data, threshold=0.03):
    # テストデータ全体の再構成画像を計算
    reconstructions = model.predict(data, verbose=0)
    
    # 差異画像の計算
    diff_images = np.abs(data - reconstructions)
    
    # 差異画像の平均値で異常検出
    losses = np.mean(diff_images, axis=(1, 2, 3))
    anomalies = losses > threshold
    
    # 差異画像の最大値でホットスポット検出
    max_diff = np.max(diff_images, axis=(1, 2, 3))
    hotspots = max_diff > threshold * 2
    
    return losses, anomalies, diff_images, hotspots, reconstructions

logger.info("Detecting anomalies with difference images...")
test_losses, anomalies, diff_images, hotspots, all_reconstructions = de-tect_anomalies_with_difference(vae, X_test, threshold=0.03)

# 異常サンプルの詳細分析
def analyze_anomalies(data, reconstructions, diff_images, losses, anomalies, hotspots, num_samples=3):
    anomaly_indices = np.where(anomalies)[0]
    if len(anomaly_indices) == 0:
        logger.info("No anomalies detected.")
        return
    
    # 表示するサンプル数を調整
    num_samples = min(num_samples, len(anomaly_indices))
    selected_indices = anomaly_indices[:num_samples]
    
    plt.figure(figsize=(15, 5 * num_samples))
    
    for i, idx in enumerate(selected_indices):
        # 3列のレイアウト (原画像, 再構成画像, 差異画像)
        # 原画像
        plt.subplot(num_samples, 3, i*3+1)
        plt.imshow(data[idx].squeeze(), cmap='gray')
        plt.title(f'Original (Index: {idx})')
        plt.axis('off')
        
        # 再構成画像
        plt.subplot(num_samples, 3, i*3+2)
        plt.imshow(reconstructions[idx].squeeze(), cmap='gray')
        plt.title('Reconstructed')
        plt.axis('off')
        
        # 差異画像
        plt.subplot(num_samples, 3, i*3+3)
        plt.imshow(diff_images[idx].squeeze(), cmap='hot', vmin=0, vmax=0.5)
        plt.title(f'Loss: {losses[idx]:.4f}, Hotspot: {hotspots[idx]}')
        plt.axis('off')
        plt.colorbar(fraction=0.046, pad=0.04)
    
    plt.tight_layout()
    plt.savefig('anomaly_analysis.png')
    plt.show()

logger.info("Analyzing detected anomalies...")
analyze_anomalies(X_test, all_reconstructions, diff_images, test_losses, anomalies, hotspots)

```

QEU:FOUNDER  ： “この結果（↓）が、今回のプロジェクトの結論です。結論ですよ・・・。”

![imageANC0-4-6](/2025-07-02-QEUR23_AENC3/imageANC0-4-6.jpg) 

QEU:FOUNDER ： “どうです？この出来栄えは・・・。”

D先生： “もう感動しかありません。AUTOENCODERで、ここまで緻密な再構成画像ができるんですか・・・。それにしても、GPUパワーがないと画像サイズを上げられないのは惜しいですね。”

QEU:FOUNDER ： “これ以上、VAE関連でやりたいことはありますか？”

D先生： “ないですね。今回のテストで、**NSOARTメトリックスのSTEP1を使うことが正解だ**ということがわかりましたから。次は、やっとBONSAIに戻りますか？”

![imageANC0-4-7](/2025-07-02-QEUR23_AENC3/imageANC0-4-7.jpg) 

QEU:FOUNDER ： “（BONSAIプロジェクトに）戻れるかどうか、現時点では小生も知らん・・・。あと、LLMのモデルにおいて、Finetuneの有効性は、いよいよあやしくなってきたねえ。”

D先生： “カスタム化の有力な技術だったのに、あ～あ・・・。”

![imageANC0-4-8](/2025-07-02-QEUR23_AENC3/imageANC0-4-8.jpg) 

QEU:FOUNDER ： “世の中の動向を見ながらゆっくり進めなければならないですね。”



## ～ まとめ ～

### ・・・ 前回のつづきです ・・・

C部長 : “じゃあ、この2つのパーティー（↑）はどうなんですか？２つとも、いかにも**「50sのニオイ」**がプンプンしますが・・・。”

![imageANC0-4-9](/2025-07-02-QEUR23_AENC3/imageANC0-4-9.jpg) 

C部長 : “う～ん、なんというかな。**あの名前が「賞味期限切れ」**なんですよ。すでに人々は**「明治がベンチマークではない」**ことについて、気が付いているのではないか？それにも関わらず、「〇新」とか「新〇組」とか・・・。”

[![MOVIE1](http://img.youtube.com/vi/4S1b4uVd0II/0.jpg)](http://www.youtube.com/watch?v=4S1b4uVd0II "参院選の目標は？｜参政党の“消費税段階廃止”に「寄せてるんちゃうの？」｜石破首相「2万給付案」とれいわ「10万給付案」の違いは")

QEU:FOUNDER ： “「←のイケメン」は、もうオワリでしょう。・・・と言うか、世のため人のためにも、もう終わって欲しい。もう一方の側は、最近になって少しだけ理念に修正を入れつつあるのかな？うまく脱皮が可能であれば、まだ期待ができるかもしれない。私見だが、ちゃんとやれば経済は大丈夫です。そもそも**少子高齢化で経済が悪くなるというのは、大いなる嘘だ**と思っています。少なくとも、過去20年間は嘘だった思うよ。”

![imageANC0-4-10](/2025-07-02-QEUR23_AENC3/imageANC0-4-10.jpg) 

QEU:FOUNDER ： “G国はGDPで追い越したし、I国はぐんぐん経済が伸びている。なんなの、これ？**しきい値が47にある**の？”

C部長 : “ん？何が言いたい？”

![imageANC0-4-11](/2025-07-02-QEUR23_AENC3/imageANC0-4-11.jpg) 

QEU:FOUNDER ： “**そもそも周り（社会）に不便を強いながら、目先の利潤を求めすぎ**なんですよ。その目的がねえ・・・。”

![imageANC0-4-12](/2025-07-02-QEUR23_AENC3/imageANC0-4-12.jpg) 

QEU:FOUNDER ： “この記事（↑）を逆の角度から見るとわかる。最も安価な政策で目先を利潤を挙げれば、当分は経営者は安泰。そんなやりかたが、**長い間続きすぎた**んだって・・・。”

C部長 : “そして、社会への押しつけ役は、皆さま大好きな、このお方（↓）ですね！！”

![imageANC0-4-13](/2025-07-02-QEUR23_AENC3/imageANC0-4-13.jpg) 

QEU:FOUNDER ： “社長様は、喜んでいるんだろうねえ。ああ、ワシは楽だ、ラクチンだ。あの人が、（立場が弱い人たちに）押し付けてくれる・・・。その一方で、同時期に、海の向こうで真に革新的な会社がなにをやってきたかというと・・・。”

![imageANC0-4-14](/2025-07-02-QEUR23_AENC3/imageANC0-4-14.jpg) 

QEU:FOUNDER ： “最も優秀なエンジニア達に、一日中ゲームをやらせたんです。もちろん、これは強化学習と言う意味だろうがね。”

![imageANC0-4-15](/2025-07-02-QEUR23_AENC3/imageANC0-4-15.jpg) 

C部長 : “すごい経営の胆力ですね。とうとう、その怠けて来たツケが回ってきたようです。”

![imageANC0-4-16](/2025-07-02-QEUR23_AENC3/imageANC0-4-16.jpg) 

QEU:FOUNDER ： “なぜ？このタイミングでこのニュースが？”

C部長 : “**長い間何もしなかった。だから、ダメになった！！でも、J国スゴイ！！**”
