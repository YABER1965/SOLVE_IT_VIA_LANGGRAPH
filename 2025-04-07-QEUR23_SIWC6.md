---
title: QEUR23_SIWC6 - 閑話休題～ドキュメントとWeb検索を使い分ける
date: 2025-04-07
tags: ["QEUシステム", "メトリックス", "Python言語", "Unsloth", "LLM", "データセット", "BONSAI", "LangGraph"]
excerpt: あたらしいLLMの学習体系を確立する
---

## QEUR23_SIWC6 - 閑話休題～ドキュメントとWeb検索を使い分ける

## ～ ちょこっと「たたき台」を作ってみた・・・ ～


D先生： “今回は、langGRAPHを使った**「閑話休題：マルチエージェントを作ってみよう！」**の最後です。今回は最終回で、小さなマルチエージェントを作るんでしたよね。ここで、参考に、前回に作成したグラフ図（↓）です。”

![imageSWC1-7-1](/2025-04-07-QEUR23_SIWC6/imageSWC1-7-1.jpg) 

D先生：“なかなかに複雑な構造になっています。とくに、右下の「not supported」って何ですか？”

QEU:FOUNDER ： “右下の**「not supported」は、生成ループを示しています**。でも、わかりにくいでしょ・・・（笑）？そこらへんは、あとで詳細に見てみましょう。それでは、これから、この「非常に簡単なマルチ・エージェント」システムを構築しましょう。その前に、ベクトルストアの中の情報を追加するために、URLリストをもう少しだけ追加しました。このリストの運用のやり方は、以前の記事のプログラムをみてください。”

```python
# ---
# データベースに取り込むURLのリスト(その2)
urls = [
    "https://jpnqeur23imbrn.blogspot.com/2025/03/qeur23bs4rvw13-solve-it-with-codeqwen.html",
    "https://jpnqeur23imbrn.blogspot.com/2025/03/qeur23bs4rvw12-solve-it-with-codeqwen.html",
    "https://jpnqeur23imbrn.blogspot.com/2025/03/qeur23bs4rvw11-solve-it-with-codeqwen.html",
    "https://jpnqeur23imbrn.blogspot.com/2025/03/qeur23bs4rvw10-solve-it-with-codeunsloth.html",
    "https://jpnqeur23imbrn.blogspot.com/2025/02/qeur23bs4rvw5-qwen-14bfinetuningunsloth.html",
    "https://jpnqeur23imbrn.blogspot.com/2025/02/qeur23-bs4rvw4-qwen-35bdeepseekdistilre.html",
    "https://jpnqeur23imbrn.blogspot.com/2025/02/qeur23qwen-35b-ftreasoningunsloth-sft.html",
    "https://jpnqeur23imbrn.blogspot.com/2025/02/qeur23bs4rvw2-qwen-35b.html",
    "https://jpnqeur23imbrn.blogspot.com/2025/01/qeur23bs4rvw1-qwen-35bqwqsimple.html",
    "https://jpnqeur23imbrn.blogspot.com/2025/01/qeur23bs4rvw0-phi4unsloth.html",
    "https://jpnqeur23imbrn.blogspot.com/2024/11/qeur23bs4brn3.html",
    "https://jpnqeur23imbrn.blogspot.com/2024/11/qeur23bs4brn2-baai-bae.html",
    "https://jpnqeur23imbrn.blogspot.com/2024/11/qeur23bs4brn1-baai-baeunsloth-inference.html",
    "https://jpnqeur23imbrn.blogspot.com/2024/11/qeur23bs3brn1-bonsai4-advanceddebate4.html",
    # ---
    "https://jpnqeur23prtm.blogspot.com/2025/02/qeur23prtm0-introduction-rt.html",
    "https://jpnqeur23prtm.blogspot.com/2025/02/qeur23prtm1-numpy.html",
    "https://jpnqeur23prtm.blogspot.com/2025/02/qeur23prtm2-simulation-data.html",
    "https://jpnqeur23prtm.blogspot.com/2025/02/qeur23prtm3-build-model.html",
    "https://jpnqeur23prtm.blogspot.com/2025/02/qeur23bs4rvw6-phi-4finetuningunsloth.html",
    "https://jpnqeur23prtm.blogspot.com/2025/02/qeur23bs4rvw7-phi-4.html",
    "https://jpnqeur23prtm.blogspot.com/2025/03/qeur23bs4rvw8-deepseekorpounsloth.html",
    # ---
    "https://jpnqeur23bonsai3twc.blogspot.com/2024/09/qeur23bonsaitwc7-question-to-question.html",
    "https://jpnqeur23bonsai3twc.blogspot.com/2024/09/qeur23bonsaitwc6-fasthtml.html",
    "https://jpnqeur23bonsai3twc.blogspot.com/2024/09/qeur23bonsaitwc5-question-to-question.html",
    "https://jpnqeur23bonsai3twc.blogspot.com/2024/09/qeur23bonsaitwc4-fasthtml.html",
    "https://jpnqeur23bonsai3twc.blogspot.com/2024/09/qeur23bonsaitwc3-question-to-question.html",
    "https://jpnqeur23bonsai3twc.blogspot.com/2024/09/qeur23bonsaitwc2-fasthtmlqa.html",
    "https://jpnqeur23bonsai3twc.blogspot.com/2024/09/qeur23bonsaitwc1-question-to-question.html",
    "https://jpnqeur23bonsai3twc.blogspot.com/2024/09/qeur23bs3twc0-bonsai3-advanceddebate2.html",
    # ---
    "https://jpnqeur23vlmapp.blogspot.com/2024/12/qeur23smnrt1.html",
    "https://jpnqeur23vlmapp.blogspot.com/2024/12/qeur23smnrt0-euclidsiamese-nn.html",
    "https://jpnqeur23vlmapp.blogspot.com/2024/12/qeur23smnw18-nsoartcsiamese-network.html",
    "https://jpnqeur23vlmapp.blogspot.com/2024/12/qeur23smnw17-nsoartc.html",
    "https://jpnqeur23vlmapp.blogspot.com/2024/11/qeur23smnw16-unslothpixtral-vlmlora.html",
    "https://jpnqeur23vlmapp.blogspot.com/2024/11/qeur23smnw14-unsloth1shot-promptqwen-vlm.html",
    "https://jpnqeur23vlmapp.blogspot.com/2024/11/qeur23smnw13-unsloth1shotvlm.html",
    "https://jpnqeur23vlmapp.blogspot.com/2024/11/qeur23smnw12-unsloth1shot.html",
    "https://jpnqeur23vlmapp.blogspot.com/2024/11/qeur23smnw11-unslothvlm.html",
    "https://jpnqeur23vlmapp.blogspot.com/2024/11/qeur23smnw10-unslothvlm.html",
    "https://jpnqeur23vlmapp.blogspot.com/2024/11/qeur23smnw4-embedding.html",
    "https://jpnqeur23vlmapp.blogspot.com/2024/11/qeur23smnw3-siamese-networkembedding.html",
    "https://jpnqeur23vlmapp.blogspot.com/2024/11/qeur23smnw2-nsoartc.html",
    "https://jpnqeur23vlmapp.blogspot.com/2024/11/qeur23smnw1-nsoartc.html",
    "https://jpnqeur23vlmapp.blogspot.com/2024/11/qeur23smnw0-introductionsiamese-network.html",
    "https://jpnqeur23vlmapp.blogspot.com/2024/10/qeur23vivlm41.html",
    "https://jpnqeur23vlmapp.blogspot.com/2024/10/qeur23vivlm40-vit-large-patch16-224.html",
    "https://jpnqeur23vlmapp.blogspot.com/2024/10/qeur23vivlm39-hf.html",
    "https://jpnqeur23vlmapp.blogspot.com/2024/10/qeur23vivlm38.html",
    "https://jpnqeur23vlmapp.blogspot.com/2024/10/qeur23vivlm37-google-vit.html",
    "https://jpnqeur23vlmapp.blogspot.com/2024/10/qeur23vivlm36-unslothvlmlora.html",
    "https://jpnqeur23vlmapp.blogspot.com/2024/10/qeur23vivlm35-lora.html",
    "https://jpnqeur23vlmapp.blogspot.com/2024/10/qeur23vivlm34-lora.html",
    "https://jpnqeur23vlmapp.blogspot.com/2024/10/qeur23vivlm33-tanshi.html",
]

```

D先生：“このURLリストの中身はなんですか？”

QEU:FOUNDER ： “外観検査関連の記事の補足が少しと、自然言語処理のBONSAIシステムの解説ですね。ここからがマルチエージェントのプログラムの開始です。まずは、序盤の準備運動からいきましょう。ドン！！”

```python
##############################
# 今回は、いままでやったことのまとめです！ 
##############################
# ---
### IMPORT PACKAGES
from typing import Literal
from pydantic import BaseModel, Field
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_fireworks import ChatFireworks

# ---
# Initialize a Fireworks chat model
llm = ChatFireworks(
    model="accounts/fireworks/models/llama-v3p3-70b-instruct",
    temperature=0.0,
    max_tokens=512
    )

# ---
from langchain_community.vectorstores import Chroma
from langchain_cohere import CohereEmbeddings

# エンべディングモデルの設定
embeddings = CohereEmbeddings(
    model="embed-multilingual-v3.0",
)

# ベクトルストアのファイルパス
VECTOR_STORE_PATH = "drive/MyDrive/chroma_db"

# ---
# 既存データベースの読み込み
try:
    vectorstore = Chroma(persist_directory=VECTOR_STORE_PATH, embed-ding_function=embeddings)
except Exception as e:
    print(f"ベクトルストアの読み込みに失敗: {e}")
    exit()

# ---
from typing import Literal
from pydantic import BaseModel, Field
from langchain_core.messages import HumanMessage, SystemMessage

# ---
# ベクトルストアの準備
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 5}  # 上位5件を取得
)

# ---
# Post-processing
def format_docs(docs):
    #doc_txt = "\n".join([doc.page_content for doc in docs])
    len_docs = len(docs)
    doc_txt = ""
    print("len_docs: ", len_docs)
    for i in range(len_docs):
        doc_txt += docs[i].page_content + "\n"
        # Check if 'source' exists in metadata
        source = docs[i].metadata.get('source', 'URL not available')
        print(f"URL: {source}\n")
    # ---
    # 不要テキストの削除
    remove_texts = [
        "スキップしてメイン コンテンツに移動",
        "このブログを検索",
        "リンクを取得",
        "Facebook", "Pinterest", "メール", "他のアプリ",
        "コメント", "コメントを投稿", "このブログの人気の投稿"
    ]
    for text in remove_texts:
        doc_txt = doc_txt.replace(text, "")
    # 改行の正規化
    doc_txt = doc_txt.replace("\n\n\n\n\n", "\n")
    doc_txt = doc_txt.replace("\n\n\n\n", "\n")
    doc_txt = doc_txt.replace("\n\n\n", "\n")
    return doc_txt.replace("\n\n", "\n").strip()

# ---
# Test用の関数
def evaluate_document(question1, question2):
    docs = retriever.invoke(question1)
    #doc_txt = docs[1].page_content
    doc_txt = format_docs(docs)
    doc_grader_prompt_formatted = doc_grader_prompt.format(document=doc_txt, ques-tion=question2)
    llm_doc_grader_input = [SystemMessage(content=doc_grader_instructions)] + [Human-Message(content=doc_grader_prompt_formatted)]
    return doc_txt, structured_llm_doc_grader.invoke(llm_doc_grader_input)

# ---
# RAG_Prompt
rag_prompt = """あなたは質問回答タスクが得意な優秀なAIアシスタントです。次の取得したコンテキスト('Context')を使用して質問に答えてください。
コンテキストの情報が不足しているため回答できない場合には、「コンテキストが不足しているためわかりません」と回答してください。
もし、あなたがその他の原因で適切な回答を生成できない場合は、「私にはわかりません」と言ってください。
最大5つの文を使用し、答えは簡潔にしてください。

Question: {question} 

Context: {context} 

Answer:"""

# ---
# Test用の関数
def rag_test_generation(question):
    # ---
    docs = retriever.invoke(question)
    doc_txt = format_docs(docs)
    rag_prompt_formatted = rag_prompt.format(context=doc_txt, question=question)
    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])
    #print(generation)
    return doc_txt, generation

# ---
# Test(1)
question = "SOART3メトリックスとは何ですか？"
docs = retriever.invoke(question)
doc_txt = format_docs(docs)
# ---
document, generation = rag_test_generation(question)
print(document)
print("-------")
print(generation)

```

QEU:FOUNDER  ： “まずは、シングルのRAGエージェントを動かします。つまり、ここまでは前回と同じです。もちろん、前回とは質問の内容を変えていますがね・・・。”

![imageSWC1-7-2](/2025-04-07-QEUR23_SIWC6/imageSWC1-7-2.jpg) 

D先生： “今回は、SOART3メトリックスに関する質問です。この質問の素材は、ベクトルストアには必ずあります。 そして、その結果は？回答の詳細を見せてください。“

**content='SOART3メトリックスとは、標準画像と計測画像の2つの画像を入力し、自己類似度メトリックスを生成する手法です。' **

additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 1022, 'total_tokens': 1060, 'completion_tokens': 38}, 'model_name': 'accounts/fireworks/models/llama-v3p3-70b-instruct', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None} 
usage_metadata={'input_tokens': 1022, 'output_tokens': 38, 'total_tokens': 1060

D先生： “もう少し詳しい説明を期待したが、ストア内の会話ベースの情報では、これ以上の詳細は無理なのでしょうね。 “

```python
# ---
### Route Query
class RouteQuery(BaseModel):
    """ 
    ユーザー クエリを最も関連性の高いデータ ソースにルーティングします。
    ユーザーの質問に応じて、Web 検索またはベクトルストアにルーティングすることを選択します。
    """
    datasource: Literal["vectorstore", "websearch"] = Field(
        ...,
        description="ユーザーの質問に応じて、Web検索('websearch')またはベクトルストア('vectorstore')にルーティングすることを選択します。",
    )

# LLM with structured output
structured_llm_router = llm.with_structured_output(RouteQuery)

# Prompt 
router_instructions = """あなたは、ユーザーの質問をベクトルストアまたはWeb検索にルーティングするエキスパートです。
このベクトルストアには、BONSAIシステム、QEUシステム、外観検査技術、合成画像の生成、SOART法(SOARTメトリックス)、NSOARTC法(NSOARTCメトリックス)、Vision Transformer(ViT)、 Siamese Neural Net(SNN)、品質管理、品質工学、タグチメソッド、政治、自民党、BRICS、パターナリズムに関連するドキュメントが含まれています。
これらのトピックに関する質問にはベクトルストアを使用してください。それ以外の場合は、Web 検索を使用してください。"""

# ---
### Documents Grader 
class GradeDocuments(BaseModel):
    """取得したドキュメントの関連性チェックのためのバイナリ・スコア。"""
    binary_score: str = Field(description="文書は質問に関連している, 'YES' or 'NO'")

# LLM with structured output
structured_llm_doc_grader = llm.with_structured_output(GradeDocuments)

# Doc grader instructions 
doc_grader_instructions = """あなたは、取得したドキュメントとユーザーの質問との関連性を評価する採点者です。
ドキュメントに質問に関連するキーワードまたは意味論的な意味(semantic meaning)が含まれている場合は、関連性があると採点します。
ドキュメントが質問に関連しているかどうかを示すために、「YES」または「NO」のバイナリ・スコアを付けてください。"""

# Grader prompt
doc_grader_prompt = "取得した文書はこちらです: \n\n {document} \n\n ユーザーの質問はこちらです: \n\n {question}"

# ---
### Hallucination Grader 
class GradeHallucinations(BaseModel):
    ### 生成回答に存在する幻覚のバイナリ スコア。
    binary_score: str = Field(description="生徒の回答は事実に基づいているか?, 'YES' or 'NO'")
    explanation: str = Field(description="あなたがスコア(YES/NO)を設定した合理的な理由を英語で説明しなさい。")

# LLM with function call 
structured_llm_hallucination_grader = llm.with_structured_output(GradeHallucinations)

# Hallucination grader instructions 
hallucination_grader_instructions = """あなたはクイズを採点する教師です。事実と生徒の回答が与えられます。
採点基準は次のとおりです:
(1) 生徒の回答が事実に基づいていることを確認しなさい。
(2) 生徒の回答に事実の範囲外の「幻覚」情報が含まれていないことを確認しなさい。

スコア(YES/NO):
YESは、生徒の回答がすべての基準を満たしていることを意味します。
NOは、生徒の回答がすべての基準を満たしていないことを意味します。

あなたは、質問から結論（スコア）を生成するにあたって、事前に合理的な判定理由である推論を生成しなければなりません。
その推論と結論が正しいことを確認するために、推論を段階的に説明してください。最初に単に正しい答えを述べることは避けてください。"""

# Grader prompt
hallucination_grader_prompt = "事実: \n\n {documents} \n\n 生徒の回答: {generation}"

# ---
### Answer Grader 
class GradeAnswer(BaseModel):
    ### 質問に対する回答を評価するためのバイナリ スコア。
    binary_score: str = Field(description="答えは質問に対して適切に回答しているか？, 'YES' or 'NO'")
    explanation: str = Field(description="あなたがスコア(YES/NO)を設定した合理的な理由を英語で説明しなさい。")

# ---
structured_llm_answer_grader = llm.with_structured_output(GradeAnswer)

# Answer grader instructions 
answer_grader_instructions = """あなたは、クイズを採点する教師です。質問と生徒の回答が与えられます。
採点基準は次のとおりです:
(1) 生徒の回答が簡潔で質問に関連していることを確認しなさい。
(2) 生徒の回答が質問の回答に役立つことを確認しなさい。

スコア(YES/NO):
YESは、生徒の回答がすべての基準を満たしていることを意味します。
NOは、生徒の回答がすべての基準を満たしていないことを意味します。

あなたは、質問から結論（スコア）を生成するにあたって、事前に合理的な判定理由である推論を生成しなければなりません。
その推論と結論が正しいことを確認するために、推論を段階的に説明してください。最初に単に正しい答えを述べることは避けてください。"""

# Grader prompt
answer_grader_prompt = "質問: \n\n {question} \n\n 生徒の回答: {generation}"

```

D先生： “ここまでは、前回の評価と分岐の記事のプログラムの内容と同じです。 “

QEU:FOUNDER  ： “これからグラフにするので、前回のプログラムのうち、関数とプロンプトだけが残ったんですよね。あとは、一気にいこうか・・・。”

```python
# ---
import operator
from typing_extensions import TypedDict
from typing import List, Annotated
from langchain.schema import Document
from langgraph.graph import END

# ---
# Web Search Tool
from langchain_community.tools.tavily_search import TavilySearchResults
web_search_tool = TavilySearchResults(k=3)

# ---
class GraphState(TypedDict):
    """
    Graph state is a dictionary that contains information we want to propagate to, and modify in, each graph node.
    グラフ状態は、各グラフ ノードに伝播し、各グラフ ノード内で変更する情報を含む辞書です。
    """
    question : str # User question
    generation : str # LLM generation
    web_search : str # Binary decision to run web search
    max_retries : int # Max number of retries for answer generation 
    answers : int # Number of answers generated
    loop_step: Annotated[int, operator.add] 
    documents : List[str] # List of retrieved documents

# ---
### Nodes
def retrieve(state):
    """
    Retrieve documents from vectorstore
    Vectorstoreからドキュメントを取得する
    Args:
        state (dict): The current graph state

    Returns:
        state (dict): New key added to state, documents, that contains retrieved documents
    """
    print("---RETRIEVE---")
    question = state["question"]

    # Write retrieved documents to documents key in state
    documents = retriever.invoke(question)
    return {"documents": documents}

def generate(state):
    """
    Generate answer using RAG on retrieved documents
    取得したドキュメントに対して RAG を使用して回答を生成する
    Args:
        state (dict): The current graph state

    Returns:
        state (dict): New key added to state, generation, that contains LLM generation
    """
    print("---生成する---")
    question = state["question"]
    documents = state["documents"]
    loop_step = state.get("loop_step", 0)
    # ---
    # RAG generation
    doc_txt = format_docs(documents)
    rag_prompt_formatted = rag_prompt.format(context=doc_txt, question=question)
    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])
    return {"generation": generation, "loop_step": loop_step+1}

def grade_documents(state):
    """
    Determines whether the retrieved documents are relevant to the question
    If any document is not relevant, we will set a flag to run web search
    取得したドキュメントが質問に関連しているかどうかを判断します
    関連しないドキュメントがある場合は、Web検索を実行するフラグを設定します
    Args:
        state (dict): The current graph state

    Returns:
        state (dict): Filtered out irrelevant documents and updated web_search state
    """
    print("---質問に対する文書の関連性を確認する---")
    question = state["question"]
    documents = state["documents"]
    # ---
    # Score each doc
    filtered_docs = []
    web_search = "No" 
    for d in documents:
        doc_grader_prompt_formatted = doc_grader_prompt.format(document=d.page_content, ques-tion=question)
        score = structured_llm_doc_grader.invoke([SystemMessage(content=doc_grader_instructions)] + [HumanMessage(content=doc_grader_prompt_formatted)])
        grade = score.binary_score
        # Document relevant
        if grade.lower() == "yes" or grade.lower() == "YES" or grade.lower() == "Yes":
            print("---GRADE: DOCUMENT RELEVANT---")
            filtered_docs.append(d)
        # Document not relevant
        else:
            print("---GRADE: DOCUMENT NOT RELEVANT---")
            # We do not include the document in filtered_docs
            # We set a flag to indicate that we want to run web search
            web_search = "Yes"
            continue
    return {"documents": filtered_docs, "web_search": web_search}
    
def web_search(state):
    """
    Web search based based on the question
    質問に基づいたウェブ検索
    Args:
        state (dict): The current graph state

    Returns:
        state (dict): Appended web results to documents
    """
    print("---ウェブ検索---")
    question = state["question"]
    documents = state.get("documents", [])

    # Web search
    docs = web_search_tool.invoke({"query": question})
    web_results = "\n".join([d["content"] for d in docs])
    web_results = Document(page_content=web_results)
    documents.append(web_results)
    return {"documents": documents}

### Edges
def route_question(state):
    """
    Route question to web search or RAG 
    質問をウェブ検索またはRAGにルーティングする
    Args:
        state (dict): The current graph state

    Returns:
        str: Next node to call
    """
    print("---質問のルーティング---")
    source = structured_llm_router.invoke([SystemMessage(content=router_instructions)] + [Hu-manMessage(content=state["question"])]) 
    if source.datasource == 'websearch':
        print("---ROUTE QUESTION TO WEB SEARCH---")
        return "websearch"
    elif source.datasource == 'vectorstore':
        print("---ROUTE QUESTION TO RAG---")
        return "vectorstore"

def decide_to_generate(state):
    """
    Determines whether to generate an answer, or add web search
    回答を生成するか、ウェブ検索を追加するかを決定します
    Args:
        state (dict): The current graph state

    Returns:
        str: Binary decision for next node to call
    """
    print("---グレード文書を評価する---")
    question = state["question"]
    web_search = state["web_search"]
    filtered_documents = state["documents"]

    if web_search == "Yes" or web_search == "yes" or web_search == "YES":
        # All documents have been filtered check_relevance
        # We will re-generate a new query
        print("---DECISION: すべての文書が質問に関連しているわけではありません。ウェブ検索も含め---")
        return "websearch"
    else:
        # We have relevant documents, so generate answer
        print("---DECISION: 生成する---")
        return "generate"

def grade_generation_v_documents_and_question(state):
    """
    Determines whether the generation is grounded in the document and answers question
    生成が文書に基づいているかどうかを判断するために、質問に答えなさい。
    Args:
        state (dict): The current graph state

    Returns:
        str: Decision for next node to call
    """
    print("---CHECK HALLUCINATIONS---")
    question = state["question"]
    documents = state["documents"]
    generation = state["generation"]
    max_retries = state.get("max_retries", 3) # Default to 3 if not provided

    hallucination_grader_prompt_formatted = hallucina-tion_grader_prompt.format(documents=format_docs(documents), generation=generation.content)
    score = struc-tured_llm_hallucination_grader.invoke([SystemMessage(content=hallucination_grader_instructions)] + [HumanMessage(content=hallucination_grader_prompt_formatted)])
    grade = score.binary_score

    # Check hallucination
    if grade == "yes" or grade == "Yes" or grade == "YES":
        print("---DECISION: 生成は文書に基づいている---")
        # Check question-answering
        print("---グレード生成と質問---")
        # Test using question and generation from above 
        answer_grader_prompt_formatted = answer_grader_prompt.format(question=question, genera-tion=generation.content)
        score = struc-tured_llm_answer_grader.invoke([SystemMessage(content=answer_grader_instructions)] + [Hu-manMessage(content=answer_grader_prompt_formatted)])
        grade = score.binary_score
        if grade == "yes" or grade == "Yes" or grade == "YES":
            print("---DECISION: 生成は質問に回答しています---")
            return "useful"
        elif state["loop_step"] <= max_retries:
            print("---DECISION: 生成は質問に答えていません---")
            return "not useful"
        else:
            print("---DECISION: 最大再試行回数に達しました---")
            return "max retries"  
    elif state["loop_step"] <= max_retries:
        print("---DECISION: 生成はドキュメントに基づいていません。再試行してください---")
        return "not supported"
    else:
        print("---DECISION: 最大再試行回数に達しました---")
        return "max retries"  

# ---
# Build Graph
from langgraph.graph import StateGraph
from IPython.display import Image, display

workflow = StateGraph(GraphState)

# Define the nodes
workflow.add_node("websearch", web_search) # web search
workflow.add_node("retrieve", retrieve) # retrieve
workflow.add_node("grade_documents", grade_documents) # grade documents
workflow.add_node("generate", generate) # generatae

# Build graph
workflow.set_conditional_entry_point(
    route_question,
    {
        "websearch": "websearch",
        "vectorstore": "retrieve",
    },
)
workflow.add_edge("websearch", "generate")
workflow.add_edge("retrieve", "grade_documents")
workflow.add_conditional_edges(
    "grade_documents",
    decide_to_generate,
    {
        "websearch": "websearch",
        "generate": "generate",
    },
)
workflow.add_conditional_edges(
    "generate",
    grade_generation_v_documents_and_question,
    {
        "not supported": "generate",
        "useful": END,
        "not useful": "websearch",
        "max retries": END,
    },
)

# Compile and graph display
graph = workflow.compile()
display(Image(graph.get_graph().draw_mermaid_png()))

# ---
graph_input = {"question": "QEUシステムにおけるBONSAIシステムの構成を教えてください", "max_retries": 3}
for event in graph.stream(graph_input, stream_mode="values"):
    print(event)

```

D先生： “ほう・・・。本当に、（プログラムの紹介を）「一気」に行きましたね（笑）。 見た目は、かなり複雑な内容なのでしょうが・・・。“

QEU:FOUNDER  ： “残りは、とくに説明することがないんですよ。・・・というか、回答の結果がなしで細かく説明すると、却ってわかりにくいし・・・。さてと・・・、まず最初の質問から行ってみましょう・・・。”

![imageSWC1-7-3](/2025-04-07-QEUR23_SIWC6/imageSWC1-7-3.jpg) 

D先生： “これは、ベクトルストアの中にあるドキュメントの参照例ですね。回答のメッセージの履歴を見ると、途中でLLMが種々の評価をしています。その結果、ベクトルストアの情報は妥当であると確認しているんですね。 “

QEU:FOUNDER  ： “そのあとで、さらにLLMがハルシネーションが存在していないかのチェックをしています。画像ではわかりにくいので、テキストで回答の詳細を確認してみましょう。”

**{'question': 'QEUシステムにおけるBONSAIシステムの構成を教えてください',**

 **'generation': AIMessage(content='BONSAIシステムは、ディベートを通じて情報を収集してDLモデルを学習するシステムです。BONSAIシステムでは、ある議題において2つの異なる意見を設定し、「POSITIVE(自分の意見に沿う)」と「NEGATIVE(自分の意見に沿わない)」の情報が収集されます。BONSAI4では、従来の学習されたLLMだけでなく、RAGで使用するEmbeddingモデルも学習します。Embeddingモデルの学習では、Tripletを使用する予定です。',**

 additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 1099, 'total_tokens': 1231, 'completion_tokens': 132}, 'model_name': 'accounts/fireworks/models/llama-v3p3-70b-instruct', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None},

usage_metadata={'input_tokens': 1099, 'output_tokens': 132, 'total_tokens': 1231}), 

**'web_search': 'No', **
'max_retries': 3, 
'loop_step': 1, 

'documents': [Document(metadata={'title': 'QEUR23_SMNRT18 : PyTorchとSIMPLEなモデルでEmbeddingを生成する(MNIST)', 'language': 'ja', 'source': 'https://jpnqeur23smnrt.blogspot.com/2024/12/qeur23smnrt18-pytorchsimpleembeddingmni.html'}, page_content='QEU:FOUNDER ： “いま、我々が推進している「BONSAI4」がそれです。BONSAIは、ディベートを通じて情報を収集してDLモデルを学習するんです。ここで重要なのは、BONSAIは、ある

・・・ 中略 ・・・

はBONSAIにおいては「議題(AGENDA)」にすぎません。つまり、その学習済みのEmbeddingモデルには、「自分の意図に合っているかどうか？(POSITIVE-NEGATIVE)」という評価情報が入っていません。もし、「POSITIVE-NEGATIVE」の情報がモデルに入っていれば、自分が意図しないNEGATIVEな情報の収集を排除できるんです。”')]}

D先生： “Cohere のDocument機能を使ったときのメッセージの構成とほとんど同じですが、なかなかに細かな情報がため込まれています。これは良いですね。次の事例のトライアルも楽しみです。 “

```python
# ---
graph_input = {"question": "SOART3メトリックスとは何ですか？", "max_retries": 3}
for event in graph.stream(graph_input, stream_mode="values"):
    print(event)

```

QEU:FOUNDER  ： “この事例の結果には、D先生も「おやっ？」と思うはずですよ・・・（笑）。”

![imageSWC1-7-4](/2025-04-07-QEUR23_SIWC6/imageSWC1-7-4.jpg) 

D先生： “なぜだ！？シングル・エージェントのときには、この質問にはうまい回答が出て来たのですが・・・。もっと細かく見てみましょう。“

**---DECISION: 最大再試行回数に達しました---**

**{'question': 'SOART3メトリックスとは何ですか？', **

**'generation': AIMessage(content='コンテキストが不足しているためわかりません。', additional_kwargs={}, **

response_metadata={'token_usage': {'prompt_tokens': 1682, 'total_tokens': 1695, 'comple-tion_tokens': 13}, 

'model_name': 'accounts/fireworks/models/llama-v3p3-70b-instruct', 'system_fingerprint': '', 'fin-ish_reason': 'stop', 'logprobs': None},

usage_metadata={'input_tokens': 1682, 'output_tokens': 13, 'total_tokens': 1695}), 

**'web_search': 'Yes',** 

'max_retries': 3, 'loop_step': 7, 'documents': [Document(metadata={'source': 'https://jpnqeur23vtfdev.blogspot.com/2024/02/qeur23vtfdev3-soartcstep1bsoart3.html', 'language': 'ja', 'title': 'QEUR23_VTFDEV3: SOARTCメトリックスとは何か(STEP1B:SOART3)'}, page_content='QEUR23_VTFDEV3: SOARTCメトリックスとは何か(STEP1B:SOART3)\n\n\n\n\n\nスキップしてメイン コンテンツに移動\n\n\n\n\nこのブログを検索\n\n\nQEU_ROUND2-3： Vision Transformerプラス SOART3メトリックスの更なる発展\n\n\n\nQEUR23_VTFDEV3: SOARTCメトリックスとは何か(STEP1B:SOART3)\n\n\nリンクを取得

・・・ 中略 ・・・

2§ã\x82¯ã\x83\x88ã\x81§ã\x81\x99ã\x81\x97ã\x80\x81ã\x83¡ã\x83\x88ã\x83ªã\x82¯ã\x82¹ç®¡ç\x90\x86ã\x82\x92å\x8f\x96ã\x82\x8aå\x85¥ã\x82\x8cã\x82\x8bã\x81®ã\x81¯ã\x80\x81ã\x81\x9dã\x81®å®\x9fè¡\x8cå\x8a\x9bã')]}


QEU:FOUNDER ： “なんか、ドキュメントには情報も少ないし、さらには**RAGの文字化け**も起こって「さんざん」ですよね。なぜ、そうなったのかはわかりません。”

D先生： “LLMが繰り返し作業をしているんですね。さらには、Web検索もしてくれているんですね。それにしても、LLMの**temprature=0の設定**になっているのに、繰り返し試行が意味あるんですか？”

QEU:FOUNDER： “（試行する意味が）無いとおもうよ・・・（笑）。Temprature=0の設定というのは、評価タスクでは必要ですが、生成タスクでは0.4程度にする必要があるでしょう。もっと欲を言えば、機能や試行回数ごとにLLMを変えたいよね。多分、それが失敗の原因でしょう。さて、次が最後の例題です。”

```python
# ---
graph_input = {"question": "日本国の政権運営において、自民党が長期化した結果として発生した主な問題点を3点説明してください。", "max_retries": 3}
for event in graph.stream(graph_input, stream_mode="values"):
    print(event)

```

D先生： “うわあ、すごい質問だなあ・・・。その結果は？“

![imageSWC1-7-5](/2025-04-07-QEUR23_SIWC6/imageSWC1-7-5.jpg) 

D先生： “そして、注目の回答の中身は？？ “

**---DECISION: 最大再試行回数に達しました---**

**{'question': '日本国の政権運営において、自民党が長期化した結果として発生した主な問題点を3点説明してください。',**

**'generation': AIMessage(content='日本国の政権運営において、自民党が長期化した結果として発生した主な問題点は以下の3点である。\n1. 予算管理能力の欠如: 自民党政権時代の当初予算は80兆円台で推移していたが、民主党政権になってから90兆円台まで拡大してしまった。\n2. 災害への備えの欠如: 民主党政権は、通常業務の執行も覚束ない状態であり、平常時を管理できない政権が、非常時の管理などできるはずがない。\n3. 国民に対して発信力不足: ぶらさがり取材拒否は頂けない。', **

additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 1804, 'total_tokens': 1964, 'completion_tokens': 160}, 

'model_name': 'accounts/fireworks/models/llama-v3p3-70b-instruct', 'system_fingerprint': '', 'fin-ish_reason': 'stop', 'logprobs': None}, 

usage_metadata={'input_tokens': 1804, 'output_tokens': 160, 'total_tokens': 1964}), 'web_search': 'Yes', 'max_retries': 3, 'loop_step': 7, 'documents': [Document(metadata={'source': 'https://jpnqeur23smnrt.blogspot.com/2024/12/qeur23smnrt17-embeddingsnn.html', 'title': 'QEUR23_SMNRT17 – EMBEDDINGを使ったSNNモデルの評価', 'language': 'ja'}, page_content='QEU:FOUNDER ： “「パワハラ、ミソジニー、お気持ち・・・」と・・・。J国が世界にリードする分野があってよかったですね。”\n\n\n\nQEU:FOUNDER ： “「身を切るカイカク」で税の減らすことを目指す一方で、自分たちが旗を振る万博では「お友達には大盤振る舞い」しています。彼らは半分左翼、「下半身がソ連的」ですね。”\n\nD先生 : “著名な建築家が、この点を別の視点から論じていますね。”\n\n\n\nQEU:FOUNDER ： “この場合には、「経済のソ連化」というよりも「社会のソ連化」ですね。とくに、ある地域（↓）においてはね。”\n\nC部長 : “あ～あ・・・。言っちゃった・・・。“'), Document(metadata={'source': 'https://jpnqeur23smnrt.blogspot.com/2024/12/qeur23smnrt16-tripletsnn.html', 'title': 'QEUR23_SMNRT16 – TRIPLETを使ったSNN学習をやってみた', 'lan-guage': 'ja'}, page_content='QEU:FOUNDER ： “「パワハラ、ミソジニー、お気持ち・・・」と・・・。J国が世界にリードする分野があってよかったですね。”\n\n\n\nQEU:FOUNDER ： “「身を切るカイカク」で税の減らすことを目指す一方で、自分たちが旗を振る万博では「お友達には大盤振る舞い」しています。たしかに、彼らは半分左翼、「下半身がソ連的」ですね。C部長、この前はイケメンが好きだと言ってませんでしたか？”

・・・ 中略 ・・・

17 ５．予算管理能力の欠如 ○バラマキ政策による歳出額の膨張 自民党政権時代には、当初予算は８０兆円台で推移していたが、民主党 政権になってから、９０兆円台まで拡大してしまった。 【当初予算額の推移】 年度 総額 (兆円) 構成比（国債費除く） 社会保障 （％） 交付税 （％） 公共事業 （％） 文教科学 （％） 防衛 （％） H19(自民) 82.9 34.1 24.1 11.2 8.5 7.8 H20(自民) 83.1 34.6 24.8 10.7 8.4 7.6 H21(自民) 88.5 36.4 24.3 10.4 7.8 7.0 H22(民主) 92.3 38.4 24.6 8.1 7.9 6.8 H23(民主) 92.4 40.5 23.7 7.0 7.8 6.7\n（なお、地方主権改革については、これからなので評価にいれていない。） ・1．国民に対して発信力不足（ぶらさがり取材拒否は頂けない）2．不適当な大臣が複数いて問題を起こし')]}

QEU:FOUNDER ： “なんと！我々のドキュメントの中にも使えるものがあったようです。もちろん、Web検索もしてくれていますし、なかなかにうれしい結末ですね。それでは、上記のうち、AIの答えだけを**ズームアップ**してみてみましょう。”

### 日本国の政権運営において、自民党が長期化した結果として発生した主な問題点は以下の3点である。

- 1. 予算管理能力の欠如: 自民党政権時代の当初予算は80兆円台で推移していたが、民主党政権になってから90兆円台まで拡大してしまった。
- 2. 災害への備えの欠如: 民主党政権は、通常業務の執行も覚束ない状態であり、平常時を管理できない政権が、非常時の管理などできるはずがない。
- 3. 国民に対して発信力不足: ぶらさがり取材拒否は頂けない。

D先生： “おっと！民主党のディスりですか・・・。なぜかといえば、RAG機能で、ずいぶん古い情報を引っ張ってきていますね。これはおもしろい！！なぜ、こんなに香ばしい結果に！？”

![imageSWC1-7-6](/2025-04-07-QEUR23_SIWC6/imageSWC1-7-6.jpg) 

QEU:FOUNDER ： “さあね・・・。**「LLMの幼児化」には興味はない**なあ。小生は小児科のお医者さんじゃないし・・・（笑）。我々の「閑話休題」はここまでですよ！マルチ・エージェントの面白みを満喫できたでしょ？”

[![MOVIE1](http://img.youtube.com/vi/QgAwIVB3UFU/0.jpg)](http://www.youtube.com/watch?v=QgAwIVB3UFU "松田語録：ソフトウェア知能爆発とは何か？")

D先生： “いやあ・・・、改善の余地はありありですが、総括するとマルチ・エージェントは便利ですわあ・・・。次は、いよいよ**「ミニ知能爆発のトライアル」**をやりましょう。ただし、その準備には時間がかかりますが・・・。“


## ～ まとめ ～

### ・・・ 前回のつづきです ・・・

C部長： “あれ？いまは、A国にある、この手の会社群（↓）にとっては、今回の状況は本来**「ボーナス・タイム」**のはず・・・。なんで、ここまで（株価が）下がるんだ！？”

![imageSWC1-7-7](/2025-04-07-QEUR23_SIWC6/imageSWC1-7-7.jpg) 

QEU:FOUNDER ： “そもそも、自動車っていう商売は**「海洋国家の産物」**だと思うんです。自動車は、**「大型貨物船で輸出するには、ちょうどいい大きさの高付加価値品」**です。一方、大陸国家は、自動車に対して、大きく依存しないんです。**ちゃんとした鉄道があればね・・・。**A国は、本来は大陸国家であるにも関わらず、自動車に依存し過ぎて鉄道建設に失敗した。これが本当にあるべき国造り（**大陸国家であり、かつ海洋国家である超大国**）の達成に著しく失敗した。その延長に、T大統領が活躍する現在がある。”

[![MOVIE2](http://img.youtube.com/vi/3AMH5fvSy1c/0.jpg)](http://www.youtube.com/watch?v=3AMH5fvSy1c "中国の反撃：パナマ運河の支配をめぐる戦い")

D先生：“これは、C国の勝利だったんでしょう？”

QEU:FOUNDER ： “そうと聞いています。ただし、こういう**「諍（いさか）いもの」**は、真偽がようわからんけどね・・・。そもそも、もっと根本的な問題があるんです。”

![imageSWC1-7-8](/2025-04-07-QEUR23_SIWC6/imageSWC1-7-8.jpg) 

D先生：“うわあ、**A国の海運の能力って小さい**んですね。”

QEU:FOUNDER ： “もともと、**モノの輸出を考えていなかった**からね。A国は輸入用の港は必要だが、船はいらなかったんだわ・・・。その意味でも、J国も似たり寄ったりですよ。”

![imageSWC1-7-9](/2025-04-07-QEUR23_SIWC6/imageSWC1-7-9.jpg) 

D先生：“J国は、**コスト効率を重視していたので海運能力を上げなかった**んですよね。”

![imageSWC1-7-10](/2025-04-07-QEUR23_SIWC6/imageSWC1-7-10.jpg) 

D先生：“まあ、**自動車は「単位体積（面積）当たりの付加価値」が圧倒的に高い**ので、この商売に特化したほうがロジスティック戦略は楽でしょう。・・・でも、それがなくなると、**J国の海運はどうなる**？”

QEU:FOUNDER ： “だからねえ・・・。「あ～あ・・・」って思うの・・・。**J国もA国も先見の明がなかった**よねえ。”

D先生：“昔、FOUNDERがネトウヨだったころ、その界隈のアイドル(おっさん)の本を読み漁ってましたよね。その中で、その著者は、**C国とK国の造船ドックの規模が大きすぎることを嗤っていました**。”

C部長 ：“ボクも、FOUNDERが偉そうに、ネトウヨ知識を開帳しているのを聞きました！！”

![imageSWC1-7-11](/2025-04-07-QEUR23_SIWC6/imageSWC1-7-11.jpg) 

QEU:FOUNDER ： “すんません・・・。**コスパの悪い（単位付加価値が低い）製品を動かすとき、輸送する船を大きくしないと何ともならない**んです。あの大将(↑)、そのリカバリ(MAGA)をしたい**熱い気持ち**は分かるが、本当にできるのかね？行動に移す前に、ちゃんと考えたの？”

