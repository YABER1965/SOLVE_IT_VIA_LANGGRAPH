---
title: QEUR23_SIWC27 - 閑話休題～AIの解釈可能性とファインチューニング(FT)
date: 2025-05-13
tags: ["QEUシステム", "メトリックス", "Python言語", "Unsloth", "LLM", "データセット", "BONSAI", "LangGraph"]
excerpt: あたらしいLLMの学習体系を確立する
---

## QEUR23_SIWC27 - 閑話休題～AIの解釈可能性とファインチューニング(FT)

## ～ どうなんだろうね。ReasoningモデルのFTって？ ～

D先生（設定年齢65歳）： “そういえば、まだLLMファインチューニングまでたどり着きません。何をためらっているんですか？“

![imageSWC2-27-1](/2025-05-13-QEUR23_SIWC27/imageSWC2-27-1.jpg) 

QEU:FOUNDER （設定年齢65歳）： “この動画（↓）を見ていて、ちょっと悩んだんです。”

[![MOVIE1](http://img.youtube.com/vi/vdj5LyCZWvQ/0.jpg)](http://www.youtube.com/watch?v=vdj5LyCZWvQ "AIの解釈可能性〜ダリオ・アモディの緊急性の主張")

D先生： “ほほう・・・。この論文（↓）の件ですね。一見すると、ブログの様式に見えるが・・・。“

![imageSWC2-27-2](/2025-05-13-QEUR23_SIWC27/imageSWC2-27-2.jpg) 

QEU:FOUNDER ： “コレを見て思ったのは、**「今の時代で、はたしてファインチューニング(FT)が何の役にたつのかな？」**って・・・。”

D先生： “おや？変なことを言いますねえ、FTの目的は**「LLMがカスタム情報を記憶するため」**じゃないんですか？ “

![imageSWC2-27-3](/2025-05-13-QEUR23_SIWC27/imageSWC2-27-3.jpg) 

QEU:FOUNDER ： “Dさんは、LLMに対する認識がDolly 15kデータセットの時代から進化していないようですね。”

D先生： “あれあれ？FOUNDERは、LLM推論について常々言っていたじゃないですか。「推論って、LLMの中の情報の海の中からプロンプトという柄杓で水（情報）を切り出すためのタスクなんだ」と・・・？”

![imageSWC2-27-4](/2025-05-13-QEUR23_SIWC27/imageSWC2-27-4.jpg) 

QEU:FOUNDER ： “LLMの創発的特性は、だんだん理解され始めているんです。ちょっと、ここらへんについてDEEPSEEKに質問してみた・・・。”

![imageSWC2-27-5](/2025-05-13-QEUR23_SIWC27/imageSWC2-27-5.jpg) 

QEU:FOUNDER ： “**質問：どのようにしてLLMは物事を記憶しているのか?**”

```python
from unsloth import FastLanguageModel
import torch

fourbit_models = [
    "unsloth/Qwen3-1.7B-unsloth-bnb-4bit", # Qwen 14B 2x faster
    "unsloth/Qwen3-4B-unsloth-bnb-4bit",
    "unsloth/Qwen3-8B-unsloth-bnb-4bit",
    "unsloth/Qwen3-14B-unsloth-bnb-4bit",
    "unsloth/Qwen3-32B-unsloth-bnb-4bit",

    # 4bit dynamic quants for superior accuracy and low memory use
    "unsloth/gemma-3-12b-it-unsloth-bnb-4bit",
    "unsloth/Phi-4",
    "unsloth/Llama-3.1-8B",
    "unsloth/Llama-3.2-3B",
    "unsloth/orpheus-3b-0.1-ft-unsloth-bnb-4bit" # [NEW] We support TTS models!
] # More models at https://huggingface.co/unsloth

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Qwen3-14B",
    max_seq_length = 2048,   # Context length - can be longer, but uses more memory
    load_in_4bit = True,     # 4bit uses much less memory
    load_in_8bit = False,    # A bit more accurate, uses 2x memory
    full_finetuning = False, # We have full finetuning now!
    # token = "hf_...",      # use one if using gated models
)

```

D先生： “なんで、いきなりUnslothのコードが？”

QEU:FOUNDER ： “あとでわかります。それでは、DEEPSEEKの答えを見てみましょう。”

**(DEEPSEEKの答え)**

大規模言語モデル（LLM）における「Q&A」データの記憶メカニズムは、従来のデータベース的な記録とは根本的に異なります。解釈可能性研究の進展から明らかになったLLMの記憶特性を、QuestionとAnswerの観点から説明します。

### 1. 記憶の本質：パターン抽出と特徴表現

LLMは特定のテキストを「文献として記録」するのではなく、訓練データから統計的パターンを抽出し、数十億のニューロン間の結合強度として分散表現します。QuestionとAnswerは以下のように処理されます：

- **Question**：単語間の文脈的関係性として符号化
- **Answer**：Questionとの共起確率に基づく生成パターンとして学習

Anthropicの研究で明らかになった「特徴（features）」の概念（3000万以上の概念がClaude 3 Son-netで確認）がこれを実現します。例えば「州都を尋ねる質問」は、「地理的位置」「行政区分」「名称比較」などの特徴の組み合わせとして表現されます。

### 2. QuestionとAnswerの記憶非対称性

最新の解釈可能性研究によると、QuestionとAnswerの情報処理には明確な非対称性が存在します：

- **Questionの記述情報**：
- トークン間の注意機構パターンとして分散保存
- 「回路（circuits）」と呼ばれる推論パス形成のトリガーとなる
- 例：ダラス→テキサス→州都→オースティンという推論回路

- **Answerの情報**：
- 特徴間の活性化確率分布として表現
- スパースオートエンコーダで抽出可能な「回答生成テンプレート」
- 複数の特徴の重ね合わせ（superposition）で構成される

### 3. 記憶の動的性質

LLMの記憶は静的な貯蔵ではなく、推論時の文脈に依存した動的再構成プロセスです：

1. Question入力で関連特徴が活性化
2. 注意機構が特徴間の関連性を加重
3. 特徴の組み合わせから新しい概念が創発
4. 確率分布に基づきAnswerを生成

このプロセスは、論文中で言及された「ゴールデンゲート・クロード」実験（特定特徴の人工的増幅）で実証されています。質問内容が直接「記録」されるのではなく、特徴間の結合強度が更新される点が重要です。

### 4. 記憶の限界とリスク

解釈可能性研究が指摘する記憶の特性：

- **重ね合わせ問題**：1ニューロンが複数概念を表現
- **文脈依存性**：同じQuestionでも文脈で異なるAnswerを生成
- **暗黙的バイアス**：訓練データの統計的偏りが特徴選択に影響

これらの特性により、特定のQ&Aペアを直接削除することは困難です。論文で言及された「脱獄」問題は、この記憶メカニズムの非決定性に起因します。

### 5. 今後の展開方向

解釈可能性を活用した記憶制御の可能性：

- 特徴の選択的抑制（例：危険知識に対応する特徴の無効化）
- 回路レベルの監視（推論パスのリアルタイム追跡）
- メタ特徴の抽出（「欺瞞的傾向」などの高次特性の検出）

Anthropicが目指す「AI用MRI」の実現により、Questionへの応答がどの特徴の組み合わせから生成されたかを特定可能になると期待されます。

**結論として、LLMはQuestionを推論回路のトリガーとして、Answerを特徴の組み合わせとして動的に処理します。両者は独立した「文献」ではなく、神経回路網の結合パターンとして相互依存的に記憶されるという点が、従来のデータベースとの本質的な違いです。解釈可能性研究は、この複雑な記憶メカニズムを可視化し、制御するための鍵となっています。**

D先生： “そうなんだ・・・。LLMの記憶というのは、そんなに簡単なモノでもないんですね。まあ、強いて言えば、Question側に有益な情報があればマシなのだが、それが学習されたとしても、LLMが適切に活用できるのかの保証がないのか・・・。”

QEU:FOUNDER ： “結局はプロンプトの出来次第なんです。先ほどのUnslothのプログラムを見てください。たぶん、Question側の情報は、どちらかというとtokenizer側に記憶されるされるのであろう。「・・・だろう」ですよ、あくまで・・・。”

![imageSWC2-27-6](/2025-05-13-QEUR23_SIWC27/imageSWC2-27-6.jpg) 

QEU:FOUNDER ： “さて、ここで簡単な実験をします。DEEPSEEKとQWEN-MAXに**「ダラスを含む州の州都は？」という質問**をしてみましょう。注意：両方ともREASONINGモデルです。”

**（質問）**

***アメリカ合衆国において、ダラスを含む州の州都の名前をおしえてください。***

QEU:FOUNDER ： “ここで、「アメリカ合衆国」という言葉を置いたのは、他の国でもダラスという地名がある懸念なんです。これに対する答えは、以下のとおりです。”

**(回答A1:DEEPSEEK)**

**[REASONING]**

ユーザーはダラスを含むアメリカの州とその州都について質問していますね。まず、ダラスはテキサス州の主要都市だと思い出しましょう。でも、ちょっと待ってください。確かですか？ダラスは確かにテキサス州にあると思います。テキサス州はアメリカ南部にある大きな州です。次はテキサス州の州都ですが、ダラス自体が州都ではないですよね？テキサス州の州都はオースティンだったと記憶しています。もう一度確認させてください。そうです、オースティンが州都です。つまり、ダラスはテキサス州にあり、州都はオースティン、というのが答えのはずです。

ちょっと待ってください。もしかしたら、州都があまりよく知られていない都市である場合もあるので、ユーザーは混乱しているかもしれません。例えば、ヒューストンがテキサス州の州都だと思っている人が多いかもしれませんが、実際はオースティンです。ですから、その点を明確にしておく必要があります。また、他の州にダラスという名前の都市はありますか？おそらくないと思います。オレゴン州にもダラスはありますが、ユーザーはアメリカ合衆国を指定しており、ダラスはテキサス州にあります。つまり、正解はテキサス州で、州都はオースティンです。ダラスは大都市ですが、州都はオースティンであることを明記し、明確に提示するべきです。そうすれば、ユーザーは混乱することなく正確な情報を得ることができます。

**[ANSWER]**

アメリカ合衆国においてダラスを含む州は**テキサス州**です。テキサス州の州都は**オースティン**（Austin）です。ダラスはテキサス州の主要都市の一つですが、州都ではありません。

**(回答B1:QWEN3-MAX)**

**[REASONING]**

ユーザーは、アメリカ合衆国のダラスを含む州の州都の名前を尋ねています。まず、ダラスがどの州にあるか思い出しましょう。ダラスはテキサス州の主要都市であることは知っています。つまり、州はテキサス州です。

次に、テキサス州の州都を確認する必要があります。私の記憶では、テキサス州の州都はオースティンです。でも、ちょっと待ってください。他の都市と混同していないか確認させてください。ヒューストンも大都市なので、ヒューストンだと思う人もいるかもしれませんが、違います。ヒューストンは主要都市であり、州都ではありません。

ちょっと待ってください。別の確認方法として、他の州の州都を考えてみましょう。例えば、カリフォルニア州の州都はサクラメント、ニューヨーク州の州都はアルバニーです。つまり、テキサス州…そうです、オースティンが州都です。オースティンは州都であると同時に、テクノロジーの中心地として知られ、「シリコンヒルズ」と呼ばれることもあることを思い出しましょう。

でも、ダラスが他の州にある可能性もあるか確認した方がいいでしょうか？いいえ、ダラスは間違いなくテキサス州にあります。フォートワースのような近隣の都市もテキサス州にあります。つまり、答えはオースティンです。

その通りだと思います。念のため、テキサスが州だった頃を思い出してみてください。テキサスは1845年にアメリカ合衆国に加盟し、それ以来オースティンが州都となっています。それ以前、テキサスが共和国だった頃は、州都は別の場所でしたが、州としては常にオースティンでした。

つまり、ダラスはテキサス州にあり、テキサス州の州都はオースティンです。つまり、答えはオースティンです。


**[ANSWER]**

アメリカ合衆国において、ダラスが含まれる州は**テキサス州**です。テキサス州の州都は**オースティン (Austin)** です。

D先生： “あれ？たしかに、動画の説明の流れと同じようにREASONINGが書き込まれていますね。”

QEU:FOUNDER ： “つまり、小生は、**LLMモデルのノードの活性化の流れがREASONINGに現れている**のだと思います。”

D先生： “そういうことか・・・。その仮説には一応の説得力があります。QWENって、REASONINGの文章の流れが悪いですよね。中間に、**不要なロジック上の「揺らぎ」があります**。”

QEU:FOUNDER ： “我々は2つのLLMを何度も使っているからわかるが、**DEEPSEEKのREASONINGの方がQWENのそれよりもREASONINGが簡潔で、かつ回答の出来が良い**です。QWEN3って最新モデルなのに残念ね・・・。さて、ここで、先ほどのプロンプトを少しだけ変えます。”

**(改良した質問)**

***アメリカ合衆国において、人口が約130万人の大都市であるダラスを含む州の州都の名前をおしえてください。***

D先生： “**「人口が約130万人の大都市」**という言葉で、ダラスという都市をより明確に特定したかったんですね。”

QEU:FOUNDER ： “こうすると、回答の内容がかなり変わります。ただし、**REASONINGがね**・・・。”

**(回答A2:DEEPSEEK)**

**[REASONING]**

では、この質問に取り組みましょう。ユーザーはアメリカ合衆国について質問しています。人口約130万人の大都市、ダラスについて言及しています。質問は、ダラスを含む州の州都を尋ねています。

まず、ダラスがどの州にあるかを思い出す必要があります。私の記憶では、ダラスはテキサス州の主要都市です。テキサス州はアメリカ南部にある大きな州です。でも、ちょっと待ってください。都市は州境近くにある場合もあるので、もしかしたら隣の州にあるのかもしれません。例えば、ダラスはオクラホマ州かルイジアナ州にあるのでしょうか？うーん、違います。ダラスは間違いなくテキサス州にあると思います。**ダラス・カウボーイズはテキサス州を本拠地とするNFLチーム**なので、それも納得できます。

さて、次は州都です。テキサス州の州都はダラスではありません。テキサス州の州都はオースティンであることは知っています。でも、確認させてください。大都市と州都を混同する人がいます。例えば、ニューヨーク市はニューヨーク州の州都ではなく、アルバニーです。同様に、ロサンゼルスはカリフォルニア州の州都ではなく、サクラメントです。つまり、テキサス州の州都はヒューストンやダラスではなく、オースティンです。これは正しいようです。オースティンは政治の中心地として知られ、州政府が置かれています。また、テキサス大学オースティン校も主要な機関です。

ちょっと待ってください。ダラスの人口を確認した方がいいかもしれません。ユーザーは約130万人と答えています。考えさせてください。ダラスも重要な都市ですが、実際にはヒューストンの方が大きいです。ヒューストンの人口は200万人を超えていますが、ダラスは約130万人です。これはユーザーの数字と一致しています。つまり、ダラスは確かにテキサス州にあり、州都はオースティンです。したがって、答えはテキサス州オースティンであるはずです。

ダラスという名前の主要都市を持つ州は他にないと思います。オレゴン州にもダラスはありますが、はるかに小さいです。主要なダラスはテキサス州にあります。つまり、州都はオースティンです。

**[ANSWER]**

アメリカ合衆国において、ダラスを含む州は**テキサス州**です。テキサス州の州都は**オースティン**（Austin）です。ダラスはテキサス州北中部に位置する大都市ですが、州政府が置かれているのはオースティン市となります。

### 補足:

- ダラスの人口は約130万人（2020年時点）で、テキサス州第3の都市です。
- オースティンはテキサス州中央部に位置し、文化やテクノロジー産業の中心地としても知られています。

**(回答B2:QWEN3-MAX)**

**[REASONING]**

ユーザーは、人口約130万人のダラスを含むアメリカ合衆国の州の州都について質問していますね。少し説明させてください。

まず、ダラスがどの州にあるかを確認する必要があります。私の記憶では、ダラスはテキサス州の主要都市です。でも、念のため確認ですが、他の州にダラスという名前の都市はありますか？オレゴン州にもダラスはありますが、はるかに小さいです。大都市として知られるダラスは、間違いなくテキサス州にあります。つまり、ここではテキサス州です。

さて、次は州都です。テキサス州の州都はオースティンです。テキサス州の州都はヒューストンやダラスではなくオースティンだと記憶しているので、正しいと思います。もう一度確認させてください。そうです、オースティンは1839年に州都となり、それ以来ずっと州都です。つまり、ダラスは大都市ですが、州都はオースティンです。

ちょっと待ってください。ユーザーはダラスの人口が約130万人だと言っていましたね。確認させてください。最近のデータによると、ダラスの人口は約130万人で、テキサス州および全米最大の都市の一つとなっています。これは正しいと言えるでしょう。

まとめると、ダラスはテキサス州にあり、テキサス州の州都はオースティンです。したがって、答えはオースティンです。混乱はないと思いますが、念のため、他の州にも同様の人口がいるかどうか確認しておきましょう。例えば、ロサンゼルスはカリフォルニア州にあり、州都はサクラメントです。しかし、ダラスはカリフォルニア州ではありません。したがって、答えは間違いなくテキサス州オースティンです。

**[ANSWER]**

アメリカ合衆国において、人口約130万人の大都市であるダラスを含む州は**テキサス州**です。テキサス州の州都は**オースティン (Austin)** です。

QEU:FOUNDER ： “今度はDEEPSEEKのREASONINGが変だよね。びっくりしない・・・？”

D先生： “変ですよねえ・・・。なぜ、わざわざ**NFLチームのことを類推する**必要があるのでしょうか？”

QEU:FOUNDER： “このREASONINGの揺らぎを合理的に解釈するのであれば、LLMの学習の結果として、tokenizerの記憶の中に、**「130万人=NFLチーム」と類推する**ような「特徴(feasure)」が形成されているのでしょうね。ここまでの結論です。別に、大規模なファインチューニングをしなくてよいでしょう。ユーザーが、**ちゃんとしたプロンプトを書ける**のであれば・・・。”

D先生： “LLMは、**プロンプトに依存した動的な記憶しかできない**らしい。もし、それが事実であるならば、必要な情報をプロンプトに入れるほうが確実です。しかし、インプットする量がおおくなると、これもなあ・・・。”

![imageSWC2-27-7](/2025-05-13-QEUR23_SIWC27/imageSWC2-27-7.jpg) 

QEU:FOUNDER ： “もちろん、**新QwenのFINETUNINGというのはREASONINGを含めて学習する**んです。これらの情報が、どちらにいくのか？tokenizerか、それともmodelの中か・・・。ここらへんが、よくわからないんです。Anthoropicの論文でも、実際には現実に追いついていないんです。まあ、もうちょっと工夫を続けてからFinetuningの必要性を再度検討しましょう。どっちみち、**「AIのない未来」なんか考えられない**のですから・・・。”


## ～ まとめ ～

QEU:FOUNDER ： “いよいよですかね・・・。”

![imageSWC2-27-8](/2025-05-13-QEUR23_SIWC27/imageSWC2-27-8.jpg) 

C部長： “このドクター（↑）さん、たびたび**「（A国債）デフォルト!!」**っていっていますが、どうなんでしょうね。8月の件は、**債務規模が自主的に決めた債務上限を超える**のですよね。”

![imageSWC2-27-9](/2025-05-13-QEUR23_SIWC27/imageSWC2-27-9.jpg) 

C部長： “債務上限を超えるというイベントは、いままでも何回もありました。しかし、その都度解決されています。”

![imageSWC2-27-10](/2025-05-13-QEUR23_SIWC27/imageSWC2-27-10.jpg) 

QEU:FOUNDER ： “今回ばかりは、違うのかもよ。脱ドル化により、**「A国債を誰が買ってくれるか」**という大問題になってきた。それ以前に、どこかの国が、所有している国債を売ってしまうと一巻の終わりです。もう、あの国は、**カネがない**んですよ。”

![imageSWC2-27-11](/2025-05-13-QEUR23_SIWC27/imageSWC2-27-11.jpg) 

QEU:FOUNDER ： “どこかの、**地方国立大学**と同じで・・・。”

![imageSWC2-27-12](/2025-05-13-QEUR23_SIWC27/imageSWC2-27-12.jpg) 

C部長： “（爆）・・・。スケールが違うが、その本質が同じですね。近い将来、一体どうなるのかなあ・・・。”

[![MOVIE2](http://img.youtube.com/vi/EV0HI-WF9wk/0.jpg)](http://www.youtube.com/watch?v=EV0HI-WF9wk "自民議員のひめゆりの塔冒とく発言問題などについて")

C部長： “なんか、**腐った吊り橋の上で殴り合い、蹴りあいのケンカしている感じ**だわ・・・。最近の大ニュースは、この潜在リスクを考えれば、小さいニュースに見えます。”

![imageSWC2-27-13](/2025-05-13-QEUR23_SIWC27/imageSWC2-27-13.jpg) 

QEU:FOUNDER ： “まさしく。完全に同意します。”

